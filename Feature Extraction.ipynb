{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sbo0WOgKvDZx","executionInfo":{"status":"ok","timestamp":1667371075323,"user_tz":-480,"elapsed":19572,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"fe8a4131-46ad-4c54-92b8-2b5a2bd90aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["cd \"/content/gdrive/My Drive/fyp1\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gEIcguUvEms","executionInfo":{"status":"ok","timestamp":1667371075324,"user_tz":-480,"elapsed":6,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"63438557-3a80-46da-b8f4-98ee32944768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/fyp1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ra0JoB1RvDZ3"},"outputs":[],"source":["import os\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","#import torchvision\n","#import torchvision.models as models\n","#from torch.utils.data import Dataset\n","#import torch.optim as optim\n","#from torch.optim import lr_scheduler\n","\n","from PIL import Image\n","import cv2 as cv\n","import json\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6QKrVvrvDZ5","executionInfo":{"status":"ok","timestamp":1667371078056,"user_tz":-480,"elapsed":7,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"3d153c22-30ea-4bc4-ec03-ab13c3accf82"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"markdown","source":["---\n","# 1. I3D module"],"metadata":{"id":"xh-BqKROIIiP"}},{"cell_type":"code","source":["class MaxPool3dSamePadding(nn.MaxPool3d):\n","    \n","    def compute_pad(self, dim, s):\n","        if s % self.stride[dim] == 0:\n","            return max(self.kernel_size[dim] - self.stride[dim], 0)\n","        else:\n","            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n","\n","    def forward(self, x):\n","        # compute 'same' padding\n","        (batch, channel, t, h, w) = x.size()\n","        out_t = np.ceil(float(t) / float(self.stride[0]))\n","        out_h = np.ceil(float(h) / float(self.stride[1]))\n","        out_w = np.ceil(float(w) / float(self.stride[2]))\n","        pad_t = self.compute_pad(0, t)\n","        pad_h = self.compute_pad(1, h)\n","        pad_w = self.compute_pad(2, w)\n","\n","        pad_t_f = pad_t // 2\n","        pad_t_b = pad_t - pad_t_f\n","        pad_h_f = pad_h // 2\n","        pad_h_b = pad_h - pad_h_f\n","        pad_w_f = pad_w // 2\n","        pad_w_b = pad_w - pad_w_f\n","\n","        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n","        x = F.pad(x, pad)\n","\n","        return super().forward(x)\n","    \n","class Unit3D(nn.Module):\n","\n","    def __init__(self, in_channels,\n","                 output_channels,\n","                 kernel_shape=(1, 1, 1),\n","                 stride=(1, 1, 1),\n","                 padding=0,\n","                 activation_fn=F.relu,\n","                 use_batch_norm=True,\n","                 use_bias=False,\n","                 name='unit_3d'):\n","        \n","        \"\"\"Initializes Unit3D module.\"\"\"\n","        super(Unit3D, self).__init__()\n","        \n","        self._output_channels = output_channels\n","        self._kernel_shape = kernel_shape\n","        self._stride = stride\n","        self._use_batch_norm = use_batch_norm\n","        self._activation_fn = activation_fn\n","        self._use_bias = use_bias\n","        self.name = name\n","        self.padding = padding\n","        \n","        self.conv3d = nn.Conv3d(in_channels=in_channels,\n","                                out_channels=self._output_channels,\n","                                kernel_size=self._kernel_shape,\n","                                stride=self._stride,\n","                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n","                                bias=self._use_bias)\n","        \n","        if self._use_batch_norm:\n","            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n","\n","    def compute_pad(self, dim, s):\n","        if s % self._stride[dim] == 0:\n","            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n","        else:\n","            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n","\n","            \n","    def forward(self, x):\n","        # compute 'same' padding\n","        (batch, channel, t, h, w) = x.size()\n","        out_t = np.ceil(float(t) / float(self._stride[0]))\n","        out_h = np.ceil(float(h) / float(self._stride[1]))\n","        out_w = np.ceil(float(w) / float(self._stride[2]))\n","        pad_t = self.compute_pad(0, t)\n","        pad_h = self.compute_pad(1, h)\n","        pad_w = self.compute_pad(2, w)\n","\n","        pad_t_f = pad_t // 2\n","        pad_t_b = pad_t - pad_t_f\n","        pad_h_f = pad_h // 2\n","        pad_h_b = pad_h - pad_h_f\n","        pad_w_f = pad_w // 2\n","        pad_w_b = pad_w - pad_w_f\n","\n","        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n","        x = F.pad(x, pad)   \n","\n","        x = self.conv3d(x)\n","        if self._use_batch_norm:\n","            x = self.bn(x)\n","        if self._activation_fn is not None:\n","            x = self._activation_fn(x)\n","            \n","        return x\n","\n","class InceptionModule(nn.Module):\n","    def __init__(self, in_channels, out_channels, name):\n","        super(InceptionModule, self).__init__()\n","\n","        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n","                         name=name+'/Branch_0/Conv3d_0a_1x1')\n","        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n","                          name=name+'/Branch_1/Conv3d_0a_1x1')\n","        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n","                          name=name+'/Branch_1/Conv3d_0b_3x3')\n","        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n","                          name=name+'/Branch_2/Conv3d_0a_1x1')\n","        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n","                          name=name+'/Branch_2/Conv3d_0b_3x3')\n","        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n","                                stride=(1, 1, 1), padding=0)\n","        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n","                          name=name+'/Branch_3/Conv3d_0b_1x1')\n","        self.name = name\n","\n","    def forward(self, x):    \n","        b0 = self.b0(x)\n","        b1 = self.b1b(self.b1a(x))\n","        b2 = self.b2b(self.b2a(x))\n","        b3 = self.b3b(self.b3a(x))\n","        return torch.cat([b0,b1,b2,b3], dim=1)\n","\n","class InceptionI3d(nn.Module):\n","    \"\"\"Inception-v1 I3D architecture.\n","    The model is introduced in:\n","        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n","        Joao Carreira, Andrew Zisserman\n","        https://arxiv.org/pdf/1705.07750v1.pdf.\n","    See also the Inception architecture, introduced in:\n","        Going deeper with convolutions\n","        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n","        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n","        http://arxiv.org/pdf/1409.4842v1.pdf.\n","    \"\"\"\n","\n","    # Endpoints of the model in order. During construction, all the endpoints up\n","    # to a designated `final_endpoint` are returned in a dictionary as the\n","    # second return value.\n","    VALID_ENDPOINTS = (\n","        'Conv3d_1a_7x7',\n","        'MaxPool3d_2a_3x3',\n","        'Conv3d_2b_1x1',\n","        'Conv3d_2c_3x3',\n","        'MaxPool3d_3a_3x3',\n","        'Mixed_3b',\n","        'Mixed_3c',\n","        'MaxPool3d_4a_3x3',\n","        'Mixed_4b',\n","        'Mixed_4c',\n","        'Mixed_4d',\n","        'Mixed_4e',\n","        'Mixed_4f',\n","        'MaxPool3d_5a_2x2',\n","        'Mixed_5b',\n","        'Mixed_5c',\n","        'Logits',\n","        'Predictions',\n","    )\n","\n","    def __init__(self, num_classes=400, spatial_squeeze=True,\n","                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n","        \"\"\"Initializes I3D model instance.\n","        Args:\n","          num_classes: The number of outputs in the logit layer (default 400, which\n","              matches the Kinetics dataset).\n","          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n","              before returning (default True).\n","          final_endpoint: The model contains many possible endpoints.\n","              `final_endpoint` specifies the last endpoint for the model to be built\n","              up to. In addition to the output at `final_endpoint`, all the outputs\n","              at endpoints up to `final_endpoint` will also be returned, in a\n","              dictionary. `final_endpoint` must be one of\n","              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n","          name: A string (optional). The name of this module.\n","        Raises:\n","          ValueError: if `final_endpoint` is not recognized.\n","        \"\"\"\n","\n","        if final_endpoint not in self.VALID_ENDPOINTS:\n","            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n","\n","        super(InceptionI3d, self).__init__()\n","        self._num_classes = num_classes\n","        self._spatial_squeeze = spatial_squeeze\n","        self._final_endpoint = final_endpoint\n","        self.logits = None\n","\n","        if self._final_endpoint not in self.VALID_ENDPOINTS:\n","            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n","\n","        self.end_points = {}\n","        end_point = 'Conv3d_1a_7x7'\n","        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n","                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'MaxPool3d_2a_3x3'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'Conv3d_2b_1x1'\n","        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n","                                       name=name+end_point)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'Conv3d_2c_3x3'\n","        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n","                                       name=name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'MaxPool3d_3a_3x3'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'Mixed_3b'\n","        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_3c'\n","        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'MaxPool3d_4a_3x3'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4b'\n","        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4c'\n","        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4d'\n","        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4e'\n","        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4f'\n","        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'MaxPool3d_5a_2x2'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_5b'\n","        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_5c'\n","        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Logits'\n","        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n","                                     stride=(1, 1, 1))\n","        self.dropout = nn.Dropout(dropout_keep_prob)\n","        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n","                             kernel_shape=[1, 1, 1],\n","                             padding=0,\n","                             activation_fn=None,\n","                             use_batch_norm=False,\n","                             use_bias=True,\n","                             name='logits')\n","\n","        self.build()\n","\n","\n","    def replace_logits(self, num_classes):\n","        self._num_classes = num_classes\n","        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n","                             kernel_shape=[1, 1, 1],\n","                             padding=0,\n","                             activation_fn=None,\n","                             use_batch_norm=False,\n","                             use_bias=True,\n","                             name='logits')\n","        \n","    def build(self):\n","        for k in self.end_points.keys():\n","            self.add_module(k, self.end_points[k])\n","        \n","    def forward(self, x):\n","        print(\"inside forward\")\n","        for end_point in self.VALID_ENDPOINTS:\n","            if end_point in self.end_points:\n","                print(f\"forwarding {end_point}\")\n","                x = self._modules[end_point](x) # use _modules to work with dataparallel\n","        print(f\"before logit shape {x.shape}\")\n","        x = self.logits(self.dropout(self.avg_pool(x)))\n","        print(f\"after logit shape {x.shape}\")\n","        if self._spatial_squeeze:\n","            logits = x.squeeze(3).squeeze(3)\n","            print(f\"after squeeeze shape {logits.shape}\")\n","        # logits is batch X time X classes, which is what we want to work with\n","        return logits\n","\n","    def extract_features(self, x, myEndPoint):\n","        #x (original size of frame) = (b,c,nframe,h,w):(1,3,16,240,320)\n","        #x_cropped                  = (b,c,nframe,h,w):(1,3,16,210,280)\n","\n","        #I3D feature extraction\n","        for end_point in self.VALID_ENDPOINTS:\n","            if end_point in self.end_points:\n","                x = self._modules[end_point](x)\n","            if end_point == myEndPoint:\n","                break\n","        #====================================\n","        # x         => (b,c,t,h,w):(1,1024,2,8,10)\n","        # x_cropped => (b,c,t,h,w):(1,1024,2,7,9)\n","        x = torch.norm(x, dim=2) #l2 normalization for t dimension\n","        # x         => (b,c,t,h,w):(1,1024,1,8,10)\n","        # x_cropped => (b,c,t,h,w):(1,1024,1,7,9)\n","        spatial_fea = torch.squeeze(x) \n","        # spatial_fea         => (c,h,w):(1024,8,10)\n","        # spatial_fea_cropped => (c,h,w):(1024,7,9)\n","\n","        x = F.adaptive_avg_pool2d(x, (1, 1)) #global pooling\n","        # x         => (b,c,t,h,w):(1,1024,1,1,1)\n","        # x_cropped => (b,c,t,h,w):(1,1024,1,1,1)\n","        x = torch.squeeze(x) #sequeeze batch(for ori/random-crop data) and spatial dimension\n","        # x         => (c,):(1024,)\n","        # x_cropped => (c,):(1024,)\n","        pooling_fea = x\n","        \n","        return spatial_fea, pooling_fea\n","        #============================================\n","        # spatial_fea         => (c,h,w):(1024,8,10)\n","        # spatial_fea_cropped => (c,h,w):(1024,7,9)\n","        # pooling_fea         => (c,):(1024,)\n","        # pooling_fea_cropped => (c,):(1024,)\n","        #============================================"],"metadata":{"id":"F8XjlQM4IL6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_I3D(mode='rgb', load_model='./rgb_imagenet.pt'):\n","    # setup the model\n","    if mode == 'flow':\n","        i3d = InceptionI3d(400, in_channels=2)\n","    else:\n","        i3d = InceptionI3d(400, in_channels=3)\n","    i3d.load_state_dict(torch.load(load_model))\n","    return i3d"],"metadata":{"id":"hZNMpR0SIZmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feature_extract(i3d, x, myEndPoint):\n","    #get available device\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    #pass to device available\n","    i3d = i3d.to(device)\n","    x = x.to(device)\n","\n","    i3d.eval()  # Set model to evaluate mode\n","    #x (original size of frame) = (b,c,nframe,h,w):(1,3,16,240,320)\n","    #x_cropped                  = (b,c,nframe,h,w):(1,3,16,210,280)\n","    with torch.no_grad():\n","        spatial_fea, pooling_fea = i3d.extract_features(x, myEndPoint)\n","    \n","    return spatial_fea, pooling_fea\n","    #============================================\n","    # spatial_fea         => (c,h,w):(1024,8,10)\n","    # spatial_fea_cropped => (c,h,w):(1024,7,9)\n","    # pooling_fea         => (c,):(1024,)\n","    # pooling_fea_cropped => (c,):(1024,)\n","    #============================================"],"metadata":{"id":"ZFgaaBxfIbJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"McIAz4nLsR8T"}},{"cell_type":"markdown","source":["---\n","# 2. Helper function"],"metadata":{"id":"bnRbUsOJOxyD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpF34b6ZvDZ7"},"outputs":[],"source":["def getVideoPath(dir_path):\n","    video_paths = []\n","\n","    video_names = os.listdir(dir_path) #eg. video_names -> [Robbery1.mp4, Robbery2.mp4]\n","    for video_name in video_names:\n","        video_path = os.path.join(dir_path, video_name) #eg. video_path -> ./dataset/Robbery/Robbery102_x264.mp4\n","        video_paths.append(video_path)\n","    return video_paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpJZ6K6ivDZ7"},"outputs":[],"source":["transform = transforms.Compose([\n","    #transforms.Resize((240,320)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZcCPkfUvDZ9"},"outputs":[],"source":["def extract_feature(video_path, i3d, category, randCrop_version):\n","    #get available device\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","    #================= data augmentation based on category ===============\n","    if category == \"random-crop\":\n","        #cropped size = (h,w):(210,280)\n","        #height = random starting point of height, from 0~29\n","        height_top = random.randint(0, 30) \n","        height_bottom = height_top + 210\n","        #width = random starting point of width, from 0~39\n","        width_left = random.randint(0, 40)\n","        width_right = width_left + 280\n","\n","        #random flip\n","        random_flip = random.randint(0, 1)\n","        if random_flip == 0:\n","            random_flip = False\n","        else:\n","            random_flip = True\n","    elif category == \"ori\":\n","        #original size = (h,w):(240,320)\n","        height_top = 0\n","        height_bottom = 240\n","        width_left = 0\n","        width_right = 320\n","        random_flip = False\n","    elif category == \"10-crop\":\n","        #first crop = topleft (height_top=0, width_left=0)\n","        #second crop = top right (height_top=0, width_left=40)\n","        #third crop = bottom left (height_top=30 ,width_left=0)\n","        #fourth crop = bottom right (height_top=30 ,width_left=40)\n","        #center point = (120,160)\n","        #top left corner of center region = (120-105, 160-140)\n","        #fifth crop = center (height_top=0 ,width_left=0)\n","        #6~10 = flip of 1~5\n","        tenCrop_loc = [[0,0], [0,40], [30,0], [30,40], [15,20],\n","                       [0,0], [0,40], [30,0], [30,40], [15,20]]\n","        height_top = tenCrop_loc[randCrop_version][0]\n","        height_bottom = height_top + 210\n","        width_left = tenCrop_loc[randCrop_version][1]\n","        width_right = width_left + 280\n","\n","        if randCrop_version <= 4:\n","            random_flip = False\n","        else: #for version 5~9\n","            random_flip = True\n","    \n","    f = [] #store 16-frame / 1 clip. reset to empty after feature extracted\n","    total_frame = 0 #total number of frame\n","    curr_nclip = 0   #current number of clip\n","    max_nclip = 21000 #WE ASSUME THE MAXIMUM #CLIP FOR A VIDEO IS 21000 CLIPS (enough for 3 hour long video) approximately 6GB\n","    isFirstClip = True\n","    \n","    #============== create tensor for transformed 16-frame ============\n","    if category == \"ori\":\n","        sixteen_frames = torch.randn(16,3,240,320) #store tensor that transformed from PIL image. (nframe,c,h,w):(16,3,240,320)\n","    elif category == \"random-crop\" or category == \"10-crop\":\n","        sixteen_frames = torch.randn(16,3,210,280) #store tensor that transformed from PIL image. (nframe,c,h,w):(16,3,210,280)\n","\n","    #============== load video ===============\n","    cap = cv.VideoCapture(video_path)\n","    #if video cannot open\n","    if (cap.isOpened()== False):\n","        raise Exception(\"Error opening video file\")\n","\n","    while(cap.isOpened()):\n","        # Capture frame-by-frame\n","        ret, frame = cap.read()\n","\n","        #if video not end yet OR total_frame less than 512. (minimum nframes = 32 segment * 16 frames ==> 512)\n","        if ret == True or total_frame < 512:\n","\n","            #if video end but total_frame less than 512\n","            if ret == False and total_frame < 512:\n","                frame = np.zeros((240, 320 , 3), dtype=np.uint8) #create black blank frame\n","\n","            total_frame += 1 \n","\n","            #================ crop frame ================\n","            frame = frame[height_top:height_bottom, width_left:width_right]\n","            #================ flip frame ================\n","            if random_flip == True:\n","                frame = cv.flip(frame, 1) #flip horizontally\n","\n","            #rescale pixel value from 0~255 to 0~1\n","            #frame = frame/255\n","\n","            #convert to PIL image\n","            frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n","            frame = Image.fromarray(frame)\n","\n","            #store current frame into a clip\n","            f.append(frame)\n","\n","            #================ for each 16-frames, extract I3D feature ================\n","            if len(f) == 16:\n","                for i, frame in enumerate(f):\n","                    #================ transformation ================\n","                    frame = transform(frame)\n","                    sixteen_frames[i] = frame  #torch.cat()  is time consuming !!! \n","                    \n","                frames = torch.transpose(sixteen_frames, 0, 1) #convert (nframe,c,h,w) to (c,nframe,h,w)\n","                frames = torch.unsqueeze(frames, 0) #add batch dimension --> frames = (b,c,nframe,h,w)\n","                frames = frames.to(device) #pass to device available\n","\n","                #================ extract I3D feature ================\n","                with torch.no_grad():\n","                    spatial_fea, pooling_fea = feature_extract(i3d, frames, myEndPoint=\"Mixed_5c\") \n","                    spatial_fea = spatial_fea.to(\"cpu\")\n","                    pooling_fea = pooling_fea.to(\"cpu\")\n","                    # spatial_fea (ori/random-crop) => feature with preserved spatial dimension  (c,h,w):(1024,h,w)\n","                    # pooling_fea (ori/random-crop) => feature with summarized spatial dimension (c,):(1024,)\n","                    if torch.any(torch.isnan(spatial_fea)).item() == True or torch.any(torch.isnan(pooling_fea)).item() == True:\n","                        raise Exception(\"[!] nan value exist after forward to model\")\n","                    if isFirstClip == True:\n","                        clips_spatial_fea = torch.randn((max_nclip, 1024, spatial_fea.shape[1], spatial_fea.shape[2])) #(nclip,c,h,w)\n","                        clips_pooling_fea = torch.randn((max_nclip, 1024)) #(nclip,c):(?, 1024)\n","                        isFirstClip = False\n","                    clips_spatial_fea[curr_nclip] = spatial_fea.clone().detach()\n","                    clips_pooling_fea[curr_nclip] = pooling_fea.clone().detach()   #torch.cat() is time consuming\n","\n","                curr_nclip += 1\n","\n","                if curr_nclip >= max_nclip:\n","                    raise Exception(\"[!] total number of clips exceed defined maximum number of clips\")\n","\n","                f = [] #reset list that store 16-frames\n","        #if video end AND total_frame more than 512\n","        else:\n","            if total_frame//16 != curr_nclip:\n","                raise Exception(\"[!] total_frame//16 != curr_nclip. total_frame\", total_frame, \"curr_nclip\", curr_nclip)\n","            if curr_nclip < 32:\n","                raise Exception(\"[!] Required at least 32 clips but only having\", curr_nclip)\n","            clips_spatial_fea = clips_spatial_fea[:curr_nclip, :].clone().detach() #remove unwanted memory \n","            clips_pooling_fea = clips_pooling_fea[:curr_nclip, :].clone().detach() #remove unwanted memory \n","            \n","            return clips_spatial_fea, clips_pooling_fea\n","            #clips_spatial_fea = (nclip,c,h,w)\n","            #clips_pooling_fea = (nclip,c):(?, 1024)"]},{"cell_type":"code","source":["#special extract step for long video (5~9 hour)\n","def hardcode_extract(video_path, i3d, category, randCrop_version):\n","    \"\"\"for long video, slower\"\"\"\n","    #get available device\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    #================= data augmentation based on category ===============\n","    if category == \"random-crop\":\n","        #cropped size = (h,w):(210,280)\n","        #height = random starting point of height, from 0~29\n","        height_top = random.randint(0, 30) \n","        height_bottom = height_top + 210\n","        #width = random starting point of width, from 0~39\n","        width_left = random.randint(0, 40)\n","        width_right = width_left + 280\n","\n","        #random flip\n","        random_flip = random.randint(0, 1)\n","        if random_flip == 0:\n","            random_flip = False\n","        else:\n","            random_flip = True\n","    elif category == \"ori\":\n","        #original size = (h,w):(240,320)\n","        height_top = 0\n","        height_bottom = 240\n","        width_left = 0\n","        width_right = 320\n","        random_flip = False\n","    elif category == \"10-crop\":\n","        #first crop = topleft (height_top=0, width_left=0)\n","        #second crop = top right (height_top=0, width_left=40)\n","        #third crop = bottom left (height_top=30 ,width_left=0)\n","        #fourth crop = bottom right (height_top=30 ,width_left=40)\n","        #center point = (120,160)\n","        #top left corner of center region = (120-105, 160-140)\n","        #fifth crop = center (height_top=0 ,width_left=0)\n","        #6~10 = flip of 1~5\n","        tenCrop_loc = [[0,0], [0,40], [30,0], [30,40], [15,20],\n","                       [0,0], [0,40], [30,0], [30,40], [15,20]]\n","        height_top = tenCrop_loc[randCrop_version][0]\n","        height_bottom = height_top + 210\n","        width_left = tenCrop_loc[randCrop_version][1]\n","        width_right = width_left + 280\n","\n","        if randCrop_version <= 4:\n","            random_flip = False\n","        else: #for version 5~9\n","            random_flip = True\n","\n","    #================ get gaps for 32 segment =========================\n","    if \"Normal_Videos308_x264.mp4\" in video_path:\n","        gaps = [0,  1907,  3814,  5722,  7629,  9536, 11443, 13351, 15258, 17165, 19072, 20979, 22887, 24794, 26701, 28608, 30516, 32423, 34330, 36237, 38144, 40052, 41959, 43866, 45773, 47680, 49588, 51495, 53402, 55309, 57217, 59124, 61031]\n","    elif \"Normal_Videos307_x264.mp4\" in video_path:\n","        gaps = [0,  1227,  2453,  3680,  4906,  6133,  7360,  8586,  9813, 11039, 12266, 13493, 14719, 15946, 17172, 18399, 19626, 20852, 22079, 23305, 24532, 25758, 26985, 28212, 29438, 30665, 31891, 33118, 34345, 35571, 36798, 38024, 39251]\n","    elif \"Normal_Videos633_x264.mp4\" in video_path:\n","        gaps = [0,   862,  1724,  2586,  3448,  4310,  5171,  6033,  6895,  7757, 8619,  9481, 10343, 11205, 12067, 12929, 13790, 14652, 15514, 16376, 17238, 18100, 18962, 19824, 20686, 21548, 22410, 23271, 24133, 24995, 25857, 26719, 27581]\n","\n","    f = [] #store 16-frame / 1 clip. reset to empty after feature extracted\n","    total_frame = 0 #total number of frame\n","    isFirstClip = True\n","\n","    #============== create tensor for transformed 16-frame ============\n","    if category == \"ori\":\n","        sixteen_frames = torch.randn(16,3,240,320) #(nframe,c,h,w):(16,3,240,320)\n","    elif category == \"random-crop\" or category == \"10-crop\":\n","        sixteen_frames = torch.randn(16,3,210,280) #(nframe,c,h,w):(16,3,210,280)\n","\n","    #============== load video =============\n","    cap = cv.VideoCapture(video_path)\n","    #if video cannot open\n","    if (cap.isOpened()== False):\n","        raise Exception(\"Error opening video file\")\n","    #while video is opened\n","    while(cap.isOpened()):\n","        # Capture frame-by-frame\n","        ret, frame = cap.read()\n","\n","        #if video not end yet \n","        if ret == True:\n","            total_frame += 1 \n","\n","            #============= crop frame ================\n","            frame = frame[height_top:height_bottom, width_left:width_right]\n","            #============= flip frame ================\n","            if random_flip == True:\n","                frame = cv.flip(frame, 1) #flip horizontally\n","\n","            #rescale pixel value from 0~255 to 0~1\n","            #frame = frame/255\n","\n","            #============= convert to PIL image ===========\n","            frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n","            frame = Image.fromarray(frame)\n","\n","            #store current frame into a clip\n","            f.append(frame)\n","\n","            #===============for each 16-frames, extract I3D feature ===================\n","            if len(f) == 16:\n","                #=============== transformation for 16-frames =====================\n","                for i, frame in enumerate(f):\n","                    #transformation\n","                    frame = transform(frame)\n","                    sixteen_frames[i] = frame  #torch.cat()  is time consuming !!! \n","                    \n","                frames = torch.transpose(sixteen_frames, 0, 1) #convert (nframe,c,h,w) to (c,nframe,h,w)\n","                frames = torch.unsqueeze(frames, 0) #add batch dimension --> frames = (b,c,nframe,h,w)\n","                \n","                #pass to device available\n","                frames = frames.to(device)\n","\n","                #================ extract clip feature (16 frames) ============\n","                with torch.no_grad():\n","                    spatial_fea, pooling_fea = feature_extract(i3d, frames, myEndPoint=\"Mixed_5c\") \n","                    # spatial_fea  => feature with preserved spatial dimension  (c,h,w):(1024,h,w)\n","                    # pooling_fea  => feature with summarized spatial dimension (c,):(1024,)\n","                    spatial_fea = spatial_fea.to(\"cpu\")\n","                    pooling_fea = pooling_fea.to(\"cpu\")\n","                    \n","                if torch.any(torch.isnan(spatial_fea)).item() == True or torch.any(torch.isnan(pooling_fea)).item() == True:\n","                    raise Exception(\"[!] nan value exist after forward to model\")\n","                #=================== create tensor based on the spatial dimension of x ====================\n","                if isFirstClip == True:\n","                    #create small tensor for nclip for 1 segment\n","                    max_nclip = 2000 #assume maximum nclip for 1 segment is 2000\n","                    curr_clip_idx = 0\n","                    curr_nclip = 0\n","\n","                    clips_spatial_fea = torch.randn((max_nclip, 1024, spatial_fea.shape[1], spatial_fea.shape[2])) #(nclip,c,h,w)\n","                    clips_pooling_fea = torch.randn((max_nclip, 1024)) #(nclip,c):(?, 1024)\n","\n","                    segm_spatial_fea = torch.randn((32,1024,spatial_fea.shape[1], spatial_fea.shape[2])) #(nsegment,c,h,w)\n","                    segm_pooling_fea = torch.randn((32,1024)) #(nsegment,c)\n","                    \n","                    isFirstClip = False\n","                clips_spatial_fea[curr_clip_idx] = spatial_fea.clone().detach()\n","                clips_pooling_fea[curr_clip_idx] = pooling_fea.clone().detach()   #torch.cat() is time consuming\n","                curr_clip_idx += 1 \n","                curr_nclip += 1\n","                #=============== when nclip is enough for 1 segment, convert clips to 1 segment ===================\n","                if curr_nclip in gaps and curr_nclip != 0:\n","                    idx = gaps.index(curr_nclip)-1\n","                    #========= segment for spatial feature ==========\n","                    x = clips_spatial_fea[0:curr_clip_idx,:] # x = (nclip,c,h,w)\n","                    segm_spatial_fea[idx] = x.mean(dim=0)\n","\n","                    #========= segment for pooling feature ==========\n","                    x = clips_pooling_fea[0:curr_clip_idx,:] # x = (nclip,c)\n","                    segm_pooling_fea[idx] = x.mean(dim=0)\n","\n","                    #reset idx to 0\n","                    curr_clip_idx = 0\n","\n","                if curr_clip_idx > max_nclip:\n","                    raise Exception(\"[!] total number of clips exceed defined maximum number of clips\")\n","\n","                f = [] #reset list that store 16-frames\n","\n","        else: #video end\n","            if total_frame//16 != curr_nclip:\n","                raise Exception(\"[!] total_frame//16 != curr_nclip. total_frame\", total_frame, \"curr_nclip\", curr_nclip)\n","            print(\"total clips:\", curr_nclip, end=\"\\t\")\n","            segm_spatial_fea = segm_spatial_fea.permute(1,0,2,3) #convert (nsegment,c,h,w) to (c,nsegment,h,w)\n","            segm_pooling_fea = segm_pooling_fea.permute(1,0)     #convert (nsegment,c) to (c,nsegment)\n","            return segm_spatial_fea, segm_pooling_fea\n","            #segm_spatial_fea = (c,nsegment,h,w):(1024,32,?,?)\n","            #segm_pooling_fea = (c,nsegment):(1024, 32)\n"],"metadata":{"id":"-NE38uAQyu-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to32_segments(x_spatial, x):\n","    \"\"\"\n","    For ori/random-crop:\n","    x = (nclips, c):(?,1024)\n","    output segments_pooling_fea = (c,nsegments):(1024,32)\n","    \n","    x_spatial = (nclip,c,h,w):(?,1024,?,?)\n","    output segments_spatial_fea = (c,nsegment,h,w):(1024,32,?,?)\"\"\"\n","\n","    gaps = torch.round(torch.linspace(0, x.shape[0], 32+1)) #from 0 to nclips-1, create 33 points will have 32 between\n","    gaps = torch.tensor(gaps, dtype=torch.int32)\n","\n","    #========== for pooling feature =================\n","    segments_pooling_fea = torch.randn(32, 1024) #(nsegment, c):(32,1024)\n","\n","    for i in range(32):\n","        #print(f\"segment {i}: clips {gaps[i]}~{gaps[i+1]-1}\")\n","        segment = x[gaps[i]:gaps[i+1],:].clone().detach() #(nclip, c), torch.tensor(x) is equivalent to x.clone().detach()\n","        segment = torch.mean(segment, 0) #mean in clip dimension, (c,):(1024)\n","        if torch.any(torch.isnan(segment)).item() == True:\n","            raise Exception(\"[!] nan value exist when seperating to 32 segments\", segment)\n","        segments_pooling_fea[i] = segment.clone().detach()\n","    segments_pooling_fea = torch.transpose(segments_pooling_fea, 0, 1) #convert (nsegment,c) to (c,nsegment):(1024,32)\n","\n","    #========== for spatial feature ===============\n","    segments_spatial_fea = torch.randn(32, 1024, x_spatial.shape[2], x_spatial.shape[3]) #(nsegment,c,h,w):(?,1024,?,?)\n","\n","    for i in range(32):\n","        #print(f\"segment {i}: clips {gaps[i]}~{gaps[i+1]-1}\")\n","        segment = x_spatial[gaps[i]:gaps[i+1],:,:,:].clone().detach() #(nclip, c,h,w)\n","        segment = torch.mean(segment, 0) #mean in clip dimension, (c,h,w):(1024,?,?)\n","        if torch.any(torch.isnan(segment)).item() == True:\n","            raise Exception(\"[!] nan value exist when seperating to 32 segments\", segment)\n","        segments_spatial_fea[i] = segment.clone().detach()\n","    segments_spatial_fea = segments_spatial_fea.permute(1,0,2,3) #convert (nsegment,c,h,w) to (c,nsegment,h,w)\n","\n","    return segments_spatial_fea, segments_pooling_fea \n","    #segments_pooling_fea = (c,nsegment):(1024, 32)\n","    #segments_spatial_fea = (c,nsegment,h,w):(1024,32,?,?)"],"metadata":{"id":"MofiUa4FXBDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createFolder(save_dir):\n","    \"\"\"This function will create folder if the folder is not created\"\"\"\n","    paths = []\n","    paths.append(save_dir)  #\"./feature_extracted/\"\n","    train_dir = os.path.join(save_dir, \"train\") #\"./feature_extracted/train/\"\n","    paths.append(train_dir) \n","    paths.append(os.path.join(train_dir, \"ori\")) #\"./feature_extracted/train/ori/\"\n","    paths.append(os.path.join(train_dir, \"ori_spatial\")) #\"./feature_extracted/train/ori_spatial/\"\n","    paths.append(os.path.join(train_dir, \"10-crop\")) #\"./feature_extracted/train/10-crop/\"\n","    paths.append(os.path.join(train_dir, \"10-crop_spatial\")) #\"./feature_extracted/train/10-crop_spatial/\"\n","    paths.append(os.path.join(train_dir, \"random-crop\")) #\"./feature_extracted/train/random-crop/\"\n","    paths.append(os.path.join(train_dir, \"random-crop_spatial\")) #\"./feature_extracted/train/random-crop_spatial/\"\n","\n","    test_dir = os.path.join(save_dir, \"test\") #\"./feature_extracted/test/\"\n","    paths.append(test_dir)\n","    paths.append(os.path.join(test_dir, \"10-crop\")) #\"./feature_extracted/test/10-crop/\"\n","    paths.append(os.path.join(test_dir, \"10-crop_spatial\")) #\"./feature_extracted/test/10-crop_spatial/\"\n","    \n","    for i in paths:\n","        if os.path.exists(i) == False: \n","            os.mkdir(i)\n","            print(\"created folder\", i)\n","        else:\n","            print(i, \"ready\")\n"],"metadata":{"id":"-lUoGHdvO89v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_save_path(video_path, save_dir, isTrain, category=None, randCrop_version=None):\n","    \"\"\"This function will return a path that the feature will be saved to\"\"\"\n","    # eg. video_path = './dataset/Shoplifting/Shoplifting021_x264.mp4'\n","    # eg. video_path = './dataset/Testing_Normal_Videos_Anomaly/Normal_Videos_003_x264.mp4'\n","    # eg. video_path = './dataset/Training-Normal-Videos-Part-1/Normal_Videos_003_x264.mp4'\n","    # eg. save_dir = \"./feature_extracted/\"\n","\n","    dir_name = os.path.basename(os.path.dirname(video_path)) #eg. Shoplifting\n","    basename = os.path.basename(video_path) #eg. Shoplifting021_x264.mp4\n","    file_name = os.path.splitext(basename)[0] #eg. Shoplifting021_x264\n","\n","    if isTrain == True:\n","        save_path = os.path.join(save_dir, \"train\", category, (file_name+\"_version\"+str(randCrop_version)+\".pt\")) # eg. \"./feature_extracted/train/random-crop/Shoplifting021_x264_version1.pt\"\n","        spatial_save_path = os.path.join(save_dir, \"train\", category+\"_spatial\", (file_name+\"_version\"+str(randCrop_version)+\".pt\")) # eg. \"./feature_extracted/train/random-crop_spatial/Shoplifting021_x264_version1.pt\"\n","    else:\n","        save_path = os.path.join(save_dir, \"test\", category, (file_name+\"_version\"+str(randCrop_version)+\".pt\")) # eg. \"./feature_extracted/test/10-crop/Shoplifting021_x264_version0.pt\"\n","        spatial_save_path = os.path.join(save_dir, \"test\", category+\"_spatial\", (file_name+\"_version\"+str(randCrop_version)+\".pt\")) # eg. \"./feature_extracted/test/10-crop_spatial/Shoplifting021_x264_version0.pt\"\n","\n","    return save_path, spatial_save_path"],"metadata":{"id":"SROaTvh5qacp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def isTrainAnomaly(video_path, anomaly_train):\n","    \"\"\"This function will return True if the video given is train data.\"\"\"\n","    #eg. video_path = './dataset/Shoplifting/Shoplifting021_x264.mp4'\n","    dir_name = os.path.basename(os.path.dirname(video_path)) #eg. Shoplifting\n","    basename = os.path.basename(video_path) #eg. Shoplifting021_x264.mp4\n","    file_name = os.path.splitext(basename)[0] #eg. Shoplifting021_x264\n","\n","    if (dir_name+\"/\"+basename+\"\\n\") in anomaly_train:\n","        return True\n","    else:\n","        return False"],"metadata":{"id":"WnGiDd6yTe_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run(video_dir, save_dir, anomaly_train_txt=None, overwrite=False, category=None, randCrop_version=0): #category = ori, 10-crop, random-crop\n","    #read path\n","    #eg. video_dir = \"./dataset/\"\n","    video_paths = getVideoPath(video_dir) \n","    #print(video_paths)\n","    print(\"===========\", category, \"version\", randCrop_version, \"=================\")\n","\n","    anomaly_train = [] # eg. anomaly_train = ['Testing_Normal_Videos_Anomaly/Normal_Videos_944_x264.mp4\\n', 'Vandalism/Vandalism007_x264.mp4\\n']\n","\n","    isAnomaly = False\n","    isTrain = False\n","    if anomaly_train_txt == \"Test_Normal\":\n","        isTrain = False\n","        print(\"normal test\")\n","    elif anomaly_train_txt == \"Train_Normal\":\n","        isTrain = True\n","        print(\"normal train\")\n","    else:\n","        isAnomaly = True\n","        #read txt to know which video is train/test\n","        f = open(anomaly_train_txt, \"r\")\n","        for line in f: \n","            anomaly_train.append(line)\n","        f.close()\n","\n","    i3d = custom_I3D()\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","    #pass to device\n","    i3d = i3d.to(device)\n","    i3d.eval()\n","    \n","    for i, video_path in enumerate(video_paths):\n","        print(f\"[{i+1}/{len(video_paths)}] {os.path.basename(video_path)}\", end=\"\\t\")\n","\n","        #====== if the video is anomaly, check whether it is train data ======\n","        if isAnomaly == True:\n","            if isTrainAnomaly(video_path, anomaly_train) == True: #if current video is train video\n","                isTrain = True\n","                print(\"anomaly train\", end=\"\\t\")\n","            else: #if current video is test video\n","                isTrain = False\n","                print(\"anomaly test\", end=\"\\t\")\n","\n","        if (isTrain == False and category == \"ori\") or (isTrain == False and category == \"random-crop\") or (isTrain == False and randCrop_version > 9): #version is only for random-crop\n","            print(\"is test data, skip\")\n","            continue \n","        \n","\n","        #================ get save path ================\n","        save_path, spatial_save_path = get_save_path(video_path, save_dir, isTrain, category, randCrop_version)\n","        \n","        #================ check whether exist ================\n","        if os.path.exists(save_path) == True and os.path.exists(spatial_save_path) == True and overwrite == False: #if existed and dont want to overwrite\n","            print(save_path, \"existed\")\n","            continue\n","        #======== read video & extract clip (16-frames) feature =============\n","        #======= feature extraction with hardcode way for nclip per segment (for long video, 5~9 hours) ============\n","        if (\"Normal_Videos308_x264.mp4\" in video_path) or (\"Normal_Videos307_x264.mp4\" in video_path) or (\"Normal_Videos633_x264.mp4\" in video_path):\n","            segments_spatial_fea, segments_pooling_fea = hardcode_extract(video_path, i3d, category, randCrop_version)\n","            #segments_spatial_fea = (c,nsegment,h,w):(1024,32,?,?)\n","            #segments_pooling_fea = (c,nsegment):(1024, 32)\n","            print(\"segments_spatial_fea:\", segments_spatial_fea.shape, end=\"\\t\")\n","            print(\"segments_pooling_fea:\", segments_pooling_fea.shape, end=\"\\t\")\n","            print(\"spatial save path:\", spatial_save_path, end=\"\\t\")\n","            print(\"save path:\", save_path)\n","            #save feature extracted to save_path \n","            torch.save(segments_spatial_fea, spatial_save_path)\n","            torch.save(segments_pooling_fea, save_path)\n","        \n","\n","        #======== feature extraction with general segment separation for train data ============\n","        elif isTrain == True:\n","            clips_spatial_fea, clips_pooling_fea = extract_feature(video_path, i3d, category, randCrop_version)\n","            #clips_spatial_fea = (nclip,c,h,w):(?,1024,?,?)\n","            #clips_pooling_fea = (nclip,c):(?, 1024)\n","\n","            #=================== seperate to 32 segments ===================\n","            segments_spatial_fea, segments_pooling_fea = to32_segments(clips_spatial_fea, clips_pooling_fea) \n","            #segments_spatial_fea = (c,nsegment,h,w):(1024,32,?,?)\n","            #segments_pooling_fea = (c,nsegment):(1024, 32)\n","\n","            print(\"segments_spatial_fea:\", segments_spatial_fea.shape, end=\"\\t\")\n","            print(\"segments_pooling_fea:\", segments_pooling_fea.shape, end=\"\\t\")\n","            print(\"spatial save path:\", spatial_save_path, end=\"\\t\")\n","            print(\"save path:\", save_path)\n","            #save feature extracted to save_path \n","            torch.save(segments_spatial_fea, spatial_save_path)\n","            torch.save(segments_pooling_fea, save_path)\n","\n","        #======== feature extraction with general segment separation for test data ============\n","        else:\n","            clips_spatial_fea, clips_pooling_fea = extract_feature(video_path, i3d, category, randCrop_version)\n","            #clips_spatial_fea = (nclip,c,h,w):(?,1024,?,?)\n","            #clips_pooling_fea = (nclip,c):(?, 1024)\n","            clips_spatial_fea = clips_spatial_fea.permute(1,0,2,3) #convert (nclip,c,h,w) to (c,nclip,h,w):(1024,?,?,?)\n","            clips_pooling_fea =  clips_pooling_fea.permute(1,0) #convert (nclip,c) to (c,nclip):(1024,?)\n","            print(\"clips_spatial_fea:\", clips_spatial_fea.shape, end=\"\\t\\t\")\n","            print(\"clips_pooling_fea:\", clips_pooling_fea.shape, end=\"\\t\")\n","            print(\"spatial save path:\", spatial_save_path, end=\"\\t\")\n","            print(\"save path:\", save_path)\n","            #save feature extracted to save_path \n","            torch.save(clips_spatial_fea, spatial_save_path)\n","            torch.save(clips_pooling_fea, save_path)\n"],"metadata":{"id":"GJtGn-nrcVOI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 3. Run"],"metadata":{"id":"ivC7ZNxsO31o"}},{"cell_type":"code","source":["createFolder(save_dir = \"./feature_extracted/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYHrefyxVHyk","executionInfo":{"status":"ok","timestamp":1667371078772,"user_tz":-480,"elapsed":8,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"e7b00f2c-ecec-401b-ea45-50040e4e245e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./feature_extracted/ ready\n","./feature_extracted/train ready\n","./feature_extracted/train/ori ready\n","./feature_extracted/train/ori_spatial ready\n","./feature_extracted/train/10-crop ready\n","./feature_extracted/train/10-crop_spatial ready\n","./feature_extracted/train/random-crop ready\n","./feature_extracted/train/random-crop_spatial ready\n","./feature_extracted/test ready\n","./feature_extracted/test/10-crop ready\n","./feature_extracted/test/10-crop_spatial ready\n"]}]},{"cell_type":"code","source":["anomaly_dir = [\"./dataset/Anomaly-Videos-Part-1/Abuse/\",\"./dataset/Anomaly-Videos-Part-1/Arrest/\",\"./dataset/Anomaly-Videos-Part-1/Arson/\", \"./dataset/Anomaly-Videos-Part-1/Assault/\",\n","             \"./dataset/Anomaly-Videos-Part-2/Burglary/\",\"./dataset/Anomaly-Videos-Part-2/Explosion/\",\"./dataset/Anomaly-Videos-Part-2/Fighting/\",\n","             \"./dataset/Anomaly-Videos-Part-3/RoadAccidents/\",\"./dataset/Anomaly-Videos-Part-3/Robbery/\",\"./dataset/Anomaly-Videos-Part-3/Shooting/\",\n","             \"./dataset/Anomaly-Videos-Part-4/Shoplifting/\",\"./dataset/Anomaly-Videos-Part-4/Stealing/\",\"./dataset/Anomaly-Videos-Part-4/Vandalism/\"]\n","train_normal_dir = [\"./dataset/Training-Normal-Videos-Part-1/\",\"./dataset/Training-Normal-Videos-Part-2/\"]\n","test_normal_dir = [\"./dataset/Testing_Normal_Videos_Anomaly/\"]"],"metadata":{"id":"ST6Lu0HlW4NW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["----\n","## 10-crop feature extraction (for train & test data)"],"metadata":{"id":"7Ot6P-v2UjQI"}},{"cell_type":"code","source":["#================= for train/test anomaly data ================\n","for i in anomaly_dir:\n","    #===== 10-crop for each video =========\n","    for j in range(10):\n","        run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", overwrite=False, category=\"10-crop\", randCrop_version=j)"],"metadata":{"id":"2UjSG6ZMUdsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=============== for train normal data ===============\n","for i in train_normal_dir:\n","    for j in range(10):\n","        run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"Train_Normal\", overwrite=False, category=\"10-crop\", randCrop_version=j)"],"metadata":{"id":"hYg-HGVFZS86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=============== for test normal data ===============\n","for i in test_normal_dir:\n","    for j in range(10):\n","        run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"Test_Normal\", overwrite=False, category=\"10-crop\", randCrop_version=j)"],"metadata":{"id":"EJ-a7JskZTX9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Original size feature extraction (for train data only)"],"metadata":{"id":"nHtxsYzVWI_a"}},{"cell_type":"code","source":["#================= for train/test anomaly data (test data will be skipped)================\n","for i in anomaly_dir:\n","    run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", overwrite=False, category=\"ori\", randCrop_version=0)"],"metadata":{"id":"DExUtP68WMPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=============== for train normal data ===============\n","for i in train_normal_dir:\n","    run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"Train_Normal\", overwrite=False, category=\"ori\", randCrop_version=0)"],"metadata":{"id":"Qn8yFbgnZXyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## random-crop feature extraction (for train data only)"],"metadata":{"id":"MQ6IBR92WUrW"}},{"cell_type":"code","source":["#================= for train/test anomaly data (test data will be skipped)================\n","#random 7version\n","for i in anomaly_dir:\n","    for j in range(7):\n","        run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", overwrite=False, category=\"random-crop\", randCrop_version=j)"],"metadata":{"id":"McChuCRcUeLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=============== for train normal data ===============\n","for i in train_normal_dir:\n","    for j in range(7):\n","        run(video_dir=i, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"Train_Normal\", overwrite=False, category=\"random-crop\", randCrop_version=j)"],"metadata":{"id":"DVv6XvxhZbrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qvA8I3zHUePg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LgpYHxZoUeTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","# 4. Check whether there are files missed"],"metadata":{"id":"rCgd6UDuj7fk"}},{"cell_type":"code","source":["def checkFile(save_dir=\"./feature_extracted/train/10-crop/\", anomaly_train_txt=\"./dataset/Anomaly_Train.txt\", maxVersion=0):\n","\n","    #================ get all video name in training data ================\n","    train_lists = []\n","    f = open(anomaly_train_txt, \"r\")\n","    for line in f: \n","        name = line.split(\"/\") #[\"Vandalism\", \"Vandalism027_x264.mp4\\n\"]\n","        basename = name[1].split(\".\") #[\"Vandalism027_x264\", \"mp4\\n\"]\n","        train_lists.append(basename[0]) #store \"Vandalism027_x264\"\n","    f.close()\n","    print(f\"require {len(train_lists)*(maxVersion+1)} files ({len(train_lists)}*(maxVersion+1))\")\n","    #================ get all saved .pt file ==============\n","    saved_names = []\n","    save_files = os.listdir(save_dir) #eg. save_files -> [Robbery1.pt, Robbery2.pt]\n","    for save_file in save_files:\n","        file_path = os.path.join(save_dir, save_file) #eg. video_path -> ./feature_extracted/Robbery102_x264.pt\n","        basename = os.path.basename(save_file) #Shoplifting021_x264.mp4\n","        file_name = os.path.splitext(basename)[0] #Shoplifting021_x264\n","        saved_names.append(file_name)\n","    print(f\"saved {len(saved_names)} files\")\n","\n","    #================ check which file is missing ====================\n","    for i in train_lists:\n","        for version in range(maxVersion+1):\n","            filename = i+str(\"_version\")+version #Vandalism027_x264_version0\n","            if filename not in saved_names: \n","                print(filename, \"is missing\")"],"metadata":{"id":"ztWabOrBsSH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#============= check for train data (anomaly & normal) =============\n","checkFile(save_dir = \"./feature_extracted/train/10-crop/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", maxVersion=9) #version 0~9"],"metadata":{"id":"g0-EWtHjOgrX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666923181888,"user_tz":-480,"elapsed":12817,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"ff720fef-13d3-45d0-da17-db7019b2c3ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["require 1610\n","saved  1610\n"]}]},{"cell_type":"code","source":["#=========== check for test data (anomaly & normal) =========\n","checkFile(save_dir = \"./feature_extracted/test/10-crop/\", anomaly_train_txt = \"./dataset/Anomaly_Test.txt\", maxVersion=9)"],"metadata":{"id":"wwaR-HhtalIK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666923166481,"user_tz":-480,"elapsed":319,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"d3cfb7cb-33eb-4a7e-8f0a-201831390f8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["require 290\n","saved  290\n"]}]},{"cell_type":"markdown","source":["---\n","# Convert groundtruth for every video into a 1D vector "],"metadata":{"id":"9AcrtZtz-iDm"}},{"cell_type":"code","source":["def write_1D_gt(test_dir, groundtruth_txt, save_path=\"./groundtruths.json\"):\n","    \"\"\"This function convert the groundtruths for all video into a 1D vector\"\"\"\n","\n","    groundtruths = np.random.randint(1, size=1113424) #total frames in test videos are 1113424\n","    next_idx = 0\n","    f = open(groundtruth_txt, \"r\")\n","    for line in f: \n","        #eg. line = Abuse028_x264.mp4  Abuse  165  240  -1  -1  \n","        annotation = line.split(\" \") #eg. annotation = ['Abuse028_x264.mp4', '', 'Abuse', '', '165', '', '240', '', '-1', '', '-1', '', '\\n']\n","        video_name = annotation[0][:-4] #Abuse028_x264\n","        print(video_name, end='\\t')\n","\n","        #load .pt to know how many frames that current video have\n","        path = os.path.join(test_dir, video_name+\"_version0.pt\")\n","        fea_clip = torch.load(path, map_location=torch.device('cpu')) #fea_clip, (c,nclip):(1024,?)\n","\n","        #get number of frames \n","        nframe = fea_clip.shape[1] * 16 \n","        print(\" fea_clip.shape\", fea_clip.shape, end='\\t')\n","        print(\"nframe\", nframe)\n","\n","        #annotation[4]~annotation[6] is anomaly (anomaly score = 1)\n","        groundtruths[next_idx+int(annotation[4])-1:next_idx+int(annotation[6])-1] = 1\n","\n","        #if the video have another anomaly moment\n","        if int(annotation[8]) != -1 and int(annotation[10]) != -1:\n","            groundtruths[next_idx+int(annotation[8])-1:next_idx+int(annotation[10])-1] = 1\n","\n","        next_idx += nframe\n","    f.close()\n","    print(\"Total nframes from all test video:\", next_idx)\n","    print(\"groundtruths.shape\", groundtruths.shape)\n","    #write list to json file\n","    with open(save_path, \"w\") as fp:\n","        json.dump(groundtruths.tolist(), fp)\n","        print(\"saved to\", save_path)\n"],"metadata":{"id":"9vmVO2-l-ro1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["write_1D_gt(test_dir=\"./feature_extracted/test/10-crop\", groundtruth_txt=\"./Temporal_Anomaly_Annotation_for_Testing_Videos.txt\", save_path=\"./groundtruths.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cj5rFzMapr6","executionInfo":{"status":"ok","timestamp":1666927512028,"user_tz":-480,"elapsed":154489,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"cea79b4e-9b66-42aa-e62e-732c337625fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Abuse028_x264\t fea_clip.shape torch.Size([10, 1024, 88])\tnframe 1408\n","Abuse030_x264\t fea_clip.shape torch.Size([10, 1024, 96])\tnframe 1536\n","Arrest001_x264\t fea_clip.shape torch.Size([10, 1024, 148])\tnframe 2368\n","Arrest007_x264\t fea_clip.shape torch.Size([10, 1024, 196])\tnframe 3136\n","Arrest024_x264\t fea_clip.shape torch.Size([10, 1024, 226])\tnframe 3616\n","Arrest030_x264\t fea_clip.shape torch.Size([10, 1024, 540])\tnframe 8640\n","Arrest039_x264\t fea_clip.shape torch.Size([10, 1024, 989])\tnframe 15824\n","Arson007_x264\t fea_clip.shape torch.Size([10, 1024, 390])\tnframe 6240\n","Arson009_x264\t fea_clip.shape torch.Size([10, 1024, 46])\tnframe 736\n","Arson010_x264\t fea_clip.shape torch.Size([10, 1024, 197])\tnframe 3152\n","Arson011_x264\t fea_clip.shape torch.Size([10, 1024, 79])\tnframe 1264\n","Arson016_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Arson018_x264\t fea_clip.shape torch.Size([10, 1024, 52])\tnframe 832\n","Arson022_x264\t fea_clip.shape torch.Size([10, 1024, 540])\tnframe 8640\n","Arson035_x264\t fea_clip.shape torch.Size([10, 1024, 89])\tnframe 1424\n","Arson041_x264\t fea_clip.shape torch.Size([10, 1024, 234])\tnframe 3744\n","Assault006_x264\t fea_clip.shape torch.Size([10, 1024, 506])\tnframe 8096\n","Assault010_x264\t fea_clip.shape torch.Size([10, 1024, 1011])\tnframe 16176\n","Assault011_x264\t fea_clip.shape torch.Size([10, 1024, 143])\tnframe 2288\n","Burglary005_x264\t fea_clip.shape torch.Size([10, 1024, 483])\tnframe 7728\n","Burglary017_x264\t fea_clip.shape torch.Size([10, 1024, 132])\tnframe 2112\n","Burglary018_x264\t fea_clip.shape torch.Size([10, 1024, 70])\tnframe 1120\n","Burglary021_x264\t fea_clip.shape torch.Size([10, 1024, 96])\tnframe 1536\n","Burglary024_x264\t fea_clip.shape torch.Size([10, 1024, 225])\tnframe 3600\n","Burglary032_x264\t fea_clip.shape torch.Size([10, 1024, 987])\tnframe 15792\n","Burglary033_x264\t fea_clip.shape torch.Size([10, 1024, 78])\tnframe 1248\n","Burglary035_x264\t fea_clip.shape torch.Size([10, 1024, 253])\tnframe 4048\n","Burglary037_x264\t fea_clip.shape torch.Size([10, 1024, 120])\tnframe 1920\n","Burglary061_x264\t fea_clip.shape torch.Size([10, 1024, 561])\tnframe 8976\n","Burglary076_x264\t fea_clip.shape torch.Size([10, 1024, 807])\tnframe 12912\n","Burglary079_x264\t fea_clip.shape torch.Size([10, 1024, 928])\tnframe 14848\n","Burglary092_x264\t fea_clip.shape torch.Size([10, 1024, 39])\tnframe 624\n","Explosion002_x264\t fea_clip.shape torch.Size([10, 1024, 250])\tnframe 4000\n","Explosion004_x264\t fea_clip.shape torch.Size([10, 1024, 118])\tnframe 1888\n","Explosion007_x264\t fea_clip.shape torch.Size([10, 1024, 1018])\tnframe 16288\n","Explosion008_x264\t fea_clip.shape torch.Size([10, 1024, 109])\tnframe 1744\n","Explosion010_x264\t fea_clip.shape torch.Size([10, 1024, 156])\tnframe 2496\n","Explosion011_x264\t fea_clip.shape torch.Size([10, 1024, 98])\tnframe 1568\n","Explosion013_x264\t fea_clip.shape torch.Size([10, 1024, 207])\tnframe 3312\n","Explosion016_x264\t fea_clip.shape torch.Size([10, 1024, 60])\tnframe 960\n","Explosion017_x264\t fea_clip.shape torch.Size([10, 1024, 102])\tnframe 1632\n","Explosion020_x264\t fea_clip.shape torch.Size([10, 1024, 80])\tnframe 1280\n","Explosion021_x264\t fea_clip.shape torch.Size([10, 1024, 48])\tnframe 768\n","Explosion022_x264\t fea_clip.shape torch.Size([10, 1024, 224])\tnframe 3584\n","Explosion025_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Explosion027_x264\t fea_clip.shape torch.Size([10, 1024, 48])\tnframe 768\n","Explosion028_x264\t fea_clip.shape torch.Size([10, 1024, 106])\tnframe 1696\n","Explosion029_x264\t fea_clip.shape torch.Size([10, 1024, 150])\tnframe 2400\n","Explosion033_x264\t fea_clip.shape torch.Size([10, 1024, 197])\tnframe 3152\n","Explosion035_x264\t fea_clip.shape torch.Size([10, 1024, 179])\tnframe 2864\n","Explosion036_x264\t fea_clip.shape torch.Size([10, 1024, 332])\tnframe 5312\n","Explosion039_x264\t fea_clip.shape torch.Size([10, 1024, 62])\tnframe 992\n","Explosion043_x264\t fea_clip.shape torch.Size([10, 1024, 477])\tnframe 7632\n","Fighting003_x264\t fea_clip.shape torch.Size([10, 1024, 193])\tnframe 3088\n","Fighting018_x264\t fea_clip.shape torch.Size([10, 1024, 86])\tnframe 1376\n","Fighting033_x264\t fea_clip.shape torch.Size([10, 1024, 69])\tnframe 1104\n","Fighting042_x264\t fea_clip.shape torch.Size([10, 1024, 139])\tnframe 2224\n","Fighting047_x264\t fea_clip.shape torch.Size([10, 1024, 278])\tnframe 4448\n","Normal_Videos_003_x264\t fea_clip.shape torch.Size([10, 1024, 176])\tnframe 2816\n","Normal_Videos_006_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_010_x264\t fea_clip.shape torch.Size([10, 1024, 65])\tnframe 1040\n","Normal_Videos_014_x264\t fea_clip.shape torch.Size([10, 1024, 93])\tnframe 1488\n","Normal_Videos_015_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_018_x264\t fea_clip.shape torch.Size([10, 1024, 73])\tnframe 1168\n","Normal_Videos_019_x264\t fea_clip.shape torch.Size([10, 1024, 177])\tnframe 2832\n","Normal_Videos_024_x264\t fea_clip.shape torch.Size([10, 1024, 67])\tnframe 1072\n","Normal_Videos_025_x264\t fea_clip.shape torch.Size([10, 1024, 37])\tnframe 592\n","Normal_Videos_027_x264\t fea_clip.shape torch.Size([10, 1024, 307])\tnframe 4912\n","Normal_Videos_033_x264\t fea_clip.shape torch.Size([10, 1024, 105])\tnframe 1680\n","Normal_Videos_034_x264\t fea_clip.shape torch.Size([10, 1024, 82])\tnframe 1312\n","Normal_Videos_041_x264\t fea_clip.shape torch.Size([10, 1024, 79])\tnframe 1264\n","Normal_Videos_042_x264\t fea_clip.shape torch.Size([10, 1024, 197])\tnframe 3152\n","Normal_Videos_048_x264\t fea_clip.shape torch.Size([10, 1024, 103])\tnframe 1648\n","Normal_Videos_050_x264\t fea_clip.shape torch.Size([10, 1024, 262])\tnframe 4192\n","Normal_Videos_051_x264\t fea_clip.shape torch.Size([10, 1024, 147])\tnframe 2352\n","Normal_Videos_056_x264\t fea_clip.shape torch.Size([10, 1024, 98])\tnframe 1568\n","Normal_Videos_059_x264\t fea_clip.shape torch.Size([10, 1024, 114])\tnframe 1824\n","Normal_Videos_063_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_067_x264\t fea_clip.shape torch.Size([10, 1024, 66])\tnframe 1056\n","Normal_Videos_070_x264\t fea_clip.shape torch.Size([10, 1024, 62])\tnframe 992\n","Normal_Videos_100_x264\t fea_clip.shape torch.Size([10, 1024, 39])\tnframe 624\n","Normal_Videos_129_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_150_x264\t fea_clip.shape torch.Size([10, 1024, 54])\tnframe 864\n","Normal_Videos_168_x264\t fea_clip.shape torch.Size([10, 1024, 108])\tnframe 1728\n","Normal_Videos_175_x264\t fea_clip.shape torch.Size([10, 1024, 552])\tnframe 8832\n","Normal_Videos_182_x264\t fea_clip.shape torch.Size([10, 1024, 255])\tnframe 4080\n","Normal_Videos_189_x264\t fea_clip.shape torch.Size([10, 1024, 46])\tnframe 736\n","Normal_Videos_196_x264\t fea_clip.shape torch.Size([10, 1024, 125])\tnframe 2000\n","Normal_Videos_203_x264\t fea_clip.shape torch.Size([10, 1024, 160])\tnframe 2560\n","Normal_Videos_210_x264\t fea_clip.shape torch.Size([10, 1024, 338])\tnframe 5408\n","Normal_Videos_217_x264\t fea_clip.shape torch.Size([10, 1024, 113])\tnframe 1808\n","Normal_Videos_224_x264\t fea_clip.shape torch.Size([10, 1024, 434])\tnframe 6944\n","Normal_Videos_246_x264\t fea_clip.shape torch.Size([10, 1024, 312])\tnframe 4992\n","Normal_Videos_247_x264\t fea_clip.shape torch.Size([10, 1024, 513])\tnframe 8208\n","Normal_Videos_248_x264\t fea_clip.shape torch.Size([10, 1024, 71])\tnframe 1136\n","Normal_Videos_251_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_289_x264\t fea_clip.shape torch.Size([10, 1024, 53])\tnframe 848\n","Normal_Videos_310_x264\t fea_clip.shape torch.Size([10, 1024, 157])\tnframe 2512\n","Normal_Videos_312_x264\t fea_clip.shape torch.Size([10, 1024, 78])\tnframe 1248\n","Normal_Videos_317_x264\t fea_clip.shape torch.Size([10, 1024, 58])\tnframe 928\n","Normal_Videos_345_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_352_x264\t fea_clip.shape torch.Size([10, 1024, 337])\tnframe 5392\n","Normal_Videos_360_x264\t fea_clip.shape torch.Size([10, 1024, 61])\tnframe 976\n","Normal_Videos_365_x264\t fea_clip.shape torch.Size([10, 1024, 414])\tnframe 6624\n","Normal_Videos_401_x264\t fea_clip.shape torch.Size([10, 1024, 101])\tnframe 1616\n","Normal_Videos_417_x264\t fea_clip.shape torch.Size([10, 1024, 67])\tnframe 1072\n","Normal_Videos_439_x264\t fea_clip.shape torch.Size([10, 1024, 264])\tnframe 4224\n","Normal_Videos_452_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_453_x264\t fea_clip.shape torch.Size([10, 1024, 332])\tnframe 5312\n","Normal_Videos_478_x264\t fea_clip.shape torch.Size([10, 1024, 281])\tnframe 4496\n","Normal_Videos_576_x264\t fea_clip.shape torch.Size([10, 1024, 704])\tnframe 11264\n","Normal_Videos_597_x264\t fea_clip.shape torch.Size([10, 1024, 139])\tnframe 2224\n","Normal_Videos_603_x264\t fea_clip.shape torch.Size([10, 1024, 204])\tnframe 3264\n","Normal_Videos_606_x264\t fea_clip.shape torch.Size([10, 1024, 77])\tnframe 1232\n","Normal_Videos_621_x264\t fea_clip.shape torch.Size([10, 1024, 300])\tnframe 4800\n","Normal_Videos_634_x264\t fea_clip.shape torch.Size([10, 1024, 841])\tnframe 13456\n","Normal_Videos_641_x264\t fea_clip.shape torch.Size([10, 1024, 225])\tnframe 3600\n","Normal_Videos_656_x264\t fea_clip.shape torch.Size([10, 1024, 113])\tnframe 1808\n","Normal_Videos_686_x264\t fea_clip.shape torch.Size([10, 1024, 150])\tnframe 2400\n","Normal_Videos_696_x264\t fea_clip.shape torch.Size([10, 1024, 226])\tnframe 3616\n","Normal_Videos_702_x264\t fea_clip.shape torch.Size([10, 1024, 157])\tnframe 2512\n","Normal_Videos_704_x264\t fea_clip.shape torch.Size([10, 1024, 105])\tnframe 1680\n","Normal_Videos_710_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Normal_Videos_717_x264\t fea_clip.shape torch.Size([10, 1024, 78])\tnframe 1248\n","Normal_Videos_722_x264\t fea_clip.shape torch.Size([10, 1024, 545])\tnframe 8720\n","Normal_Videos_725_x264\t fea_clip.shape torch.Size([10, 1024, 57])\tnframe 912\n","Normal_Videos_745_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_758_x264\t fea_clip.shape torch.Size([10, 1024, 99])\tnframe 1584\n","Normal_Videos_778_x264\t fea_clip.shape torch.Size([10, 1024, 78])\tnframe 1248\n","Normal_Videos_780_x264\t fea_clip.shape torch.Size([10, 1024, 126])\tnframe 2016\n","Normal_Videos_781_x264\t fea_clip.shape torch.Size([10, 1024, 248])\tnframe 3968\n","Normal_Videos_782_x264\t fea_clip.shape torch.Size([10, 1024, 346])\tnframe 5536\n","Normal_Videos_783_x264\t fea_clip.shape torch.Size([10, 1024, 599])\tnframe 9584\n","Normal_Videos_798_x264\t fea_clip.shape torch.Size([10, 1024, 375])\tnframe 6000\n","Normal_Videos_801_x264\t fea_clip.shape torch.Size([10, 1024, 171])\tnframe 2736\n","Normal_Videos_828_x264\t fea_clip.shape torch.Size([10, 1024, 58])\tnframe 928\n","Normal_Videos_831_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_866_x264\t fea_clip.shape torch.Size([10, 1024, 74])\tnframe 1184\n","Normal_Videos_867_x264\t fea_clip.shape torch.Size([10, 1024, 39])\tnframe 624\n","Normal_Videos_868_x264\t fea_clip.shape torch.Size([10, 1024, 150])\tnframe 2400\n","Normal_Videos_869_x264\t fea_clip.shape torch.Size([10, 1024, 150])\tnframe 2400\n","Normal_Videos_870_x264\t fea_clip.shape torch.Size([10, 1024, 37])\tnframe 592\n","Normal_Videos_871_x264\t fea_clip.shape torch.Size([10, 1024, 272])\tnframe 4352\n","Normal_Videos_872_x264\t fea_clip.shape torch.Size([10, 1024, 33])\tnframe 528\n","Normal_Videos_873_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Normal_Videos_874_x264\t fea_clip.shape torch.Size([10, 1024, 264])\tnframe 4224\n","Normal_Videos_875_x264\t fea_clip.shape torch.Size([10, 1024, 160])\tnframe 2560\n","Normal_Videos_876_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_877_x264\t fea_clip.shape torch.Size([10, 1024, 626])\tnframe 10016\n","Normal_Videos_878_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_879_x264\t fea_clip.shape torch.Size([10, 1024, 71])\tnframe 1136\n","Normal_Videos_880_x264\t fea_clip.shape torch.Size([10, 1024, 1128])\tnframe 18048\n","Normal_Videos_881_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_882_x264\t fea_clip.shape torch.Size([10, 1024, 103])\tnframe 1648\n","Normal_Videos_883_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_884_x264\t fea_clip.shape torch.Size([10, 1024, 564])\tnframe 9024\n","Normal_Videos_885_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_886_x264\t fea_clip.shape torch.Size([10, 1024, 171])\tnframe 2736\n","Normal_Videos_887_x264\t fea_clip.shape torch.Size([10, 1024, 476])\tnframe 7616\n","Normal_Videos_888_x264\t fea_clip.shape torch.Size([10, 1024, 35])\tnframe 560\n","Normal_Videos_889_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Normal_Videos_890_x264\t fea_clip.shape torch.Size([10, 1024, 223])\tnframe 3568\n","Normal_Videos_891_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Normal_Videos_892_x264\t fea_clip.shape torch.Size([10, 1024, 110])\tnframe 1760\n","Normal_Videos_893_x264\t fea_clip.shape torch.Size([10, 1024, 398])\tnframe 6368\n","Normal_Videos_894_x264\t fea_clip.shape torch.Size([10, 1024, 160])\tnframe 2560\n","Normal_Videos_895_x264\t fea_clip.shape torch.Size([10, 1024, 189])\tnframe 3024\n","Normal_Videos_896_x264\t fea_clip.shape torch.Size([10, 1024, 143])\tnframe 2288\n","Normal_Videos_897_x264\t fea_clip.shape torch.Size([10, 1024, 54])\tnframe 864\n","Normal_Videos_898_x264\t fea_clip.shape torch.Size([10, 1024, 62])\tnframe 992\n","Normal_Videos_899_x264\t fea_clip.shape torch.Size([10, 1024, 86])\tnframe 1376\n","Normal_Videos_900_x264\t fea_clip.shape torch.Size([10, 1024, 90])\tnframe 1440\n","Normal_Videos_901_x264\t fea_clip.shape torch.Size([10, 1024, 73])\tnframe 1168\n","Normal_Videos_902_x264\t fea_clip.shape torch.Size([10, 1024, 86])\tnframe 1376\n","Normal_Videos_903_x264\t fea_clip.shape torch.Size([10, 1024, 49])\tnframe 784\n","Normal_Videos_904_x264\t fea_clip.shape torch.Size([10, 1024, 56])\tnframe 896\n","Normal_Videos_905_x264\t fea_clip.shape torch.Size([10, 1024, 74])\tnframe 1184\n","Normal_Videos_906_x264\t fea_clip.shape torch.Size([10, 1024, 42])\tnframe 672\n","Normal_Videos_907_x264\t fea_clip.shape torch.Size([10, 1024, 37])\tnframe 592\n","Normal_Videos_908_x264\t fea_clip.shape torch.Size([10, 1024, 55])\tnframe 880\n","Normal_Videos_909_x264\t fea_clip.shape torch.Size([10, 1024, 54])\tnframe 864\n","Normal_Videos_910_x264\t fea_clip.shape torch.Size([10, 1024, 35])\tnframe 560\n","Normal_Videos_911_x264\t fea_clip.shape torch.Size([10, 1024, 48])\tnframe 768\n","Normal_Videos_912_x264\t fea_clip.shape torch.Size([10, 1024, 46])\tnframe 736\n","Normal_Videos_913_x264\t fea_clip.shape torch.Size([10, 1024, 38])\tnframe 608\n","Normal_Videos_914_x264\t fea_clip.shape torch.Size([10, 1024, 55])\tnframe 880\n","Normal_Videos_915_x264\t fea_clip.shape torch.Size([10, 1024, 77])\tnframe 1232\n","Normal_Videos_923_x264\t fea_clip.shape torch.Size([10, 1024, 1139])\tnframe 18224\n","Normal_Videos_924_x264\t fea_clip.shape torch.Size([10, 1024, 6749])\tnframe 107984\n","Normal_Videos_925_x264\t fea_clip.shape torch.Size([10, 1024, 482])\tnframe 7712\n","Normal_Videos_926_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Normal_Videos_927_x264\t fea_clip.shape torch.Size([10, 1024, 102])\tnframe 1632\n","Normal_Videos_928_x264\t fea_clip.shape torch.Size([10, 1024, 57])\tnframe 912\n","Normal_Videos_929_x264\t fea_clip.shape torch.Size([10, 1024, 57])\tnframe 912\n","Normal_Videos_930_x264\t fea_clip.shape torch.Size([10, 1024, 199])\tnframe 3184\n","Normal_Videos_931_x264\t fea_clip.shape torch.Size([10, 1024, 110])\tnframe 1760\n","Normal_Videos_932_x264\t fea_clip.shape torch.Size([10, 1024, 111])\tnframe 1776\n","Normal_Videos_933_x264\t fea_clip.shape torch.Size([10, 1024, 110])\tnframe 1760\n","Normal_Videos_934_x264\t fea_clip.shape torch.Size([10, 1024, 110])\tnframe 1760\n","Normal_Videos_935_x264\t fea_clip.shape torch.Size([10, 1024, 6749])\tnframe 107984\n","Normal_Videos_936_x264\t fea_clip.shape torch.Size([10, 1024, 71])\tnframe 1136\n","Normal_Videos_937_x264\t fea_clip.shape torch.Size([10, 1024, 71])\tnframe 1136\n","Normal_Videos_938_x264\t fea_clip.shape torch.Size([10, 1024, 301])\tnframe 4816\n","Normal_Videos_939_x264\t fea_clip.shape torch.Size([10, 1024, 50])\tnframe 800\n","Normal_Videos_940_x264\t fea_clip.shape torch.Size([10, 1024, 2251])\tnframe 36016\n","Normal_Videos_941_x264\t fea_clip.shape torch.Size([10, 1024, 126])\tnframe 2016\n","Normal_Videos_943_x264\t fea_clip.shape torch.Size([10, 1024, 63])\tnframe 1008\n","Normal_Videos_944_x264\t fea_clip.shape torch.Size([10, 1024, 448])\tnframe 7168\n","RoadAccidents001_x264\t fea_clip.shape torch.Size([10, 1024, 85])\tnframe 1360\n","RoadAccidents002_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","RoadAccidents004_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","RoadAccidents009_x264\t fea_clip.shape torch.Size([10, 1024, 57])\tnframe 912\n","RoadAccidents010_x264\t fea_clip.shape torch.Size([10, 1024, 33])\tnframe 528\n","RoadAccidents011_x264\t fea_clip.shape torch.Size([10, 1024, 134])\tnframe 2144\n","RoadAccidents012_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","RoadAccidents016_x264\t fea_clip.shape torch.Size([10, 1024, 137])\tnframe 2192\n","RoadAccidents017_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","RoadAccidents019_x264\t fea_clip.shape torch.Size([10, 1024, 82])\tnframe 1312\n","RoadAccidents020_x264\t fea_clip.shape torch.Size([10, 1024, 110])\tnframe 1760\n","RoadAccidents021_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","RoadAccidents022_x264\t fea_clip.shape torch.Size([10, 1024, 44])\tnframe 704\n","RoadAccidents121_x264\t fea_clip.shape torch.Size([10, 1024, 114])\tnframe 1824\n","RoadAccidents122_x264\t fea_clip.shape torch.Size([10, 1024, 40])\tnframe 640\n","RoadAccidents123_x264\t fea_clip.shape torch.Size([10, 1024, 62])\tnframe 992\n","RoadAccidents124_x264\t fea_clip.shape torch.Size([10, 1024, 93])\tnframe 1488\n","RoadAccidents125_x264\t fea_clip.shape torch.Size([10, 1024, 110])\tnframe 1760\n","RoadAccidents127_x264\t fea_clip.shape torch.Size([10, 1024, 161])\tnframe 2576\n","RoadAccidents128_x264\t fea_clip.shape torch.Size([10, 1024, 35])\tnframe 560\n","RoadAccidents131_x264\t fea_clip.shape torch.Size([10, 1024, 95])\tnframe 1520\n","RoadAccidents132_x264\t fea_clip.shape torch.Size([10, 1024, 116])\tnframe 1856\n","RoadAccidents133_x264\t fea_clip.shape torch.Size([10, 1024, 42])\tnframe 672\n","Robbery048_x264\t fea_clip.shape torch.Size([10, 1024, 88])\tnframe 1408\n","Robbery050_x264\t fea_clip.shape torch.Size([10, 1024, 106])\tnframe 1696\n","Robbery102_x264\t fea_clip.shape torch.Size([10, 1024, 114])\tnframe 1824\n","Robbery106_x264\t fea_clip.shape torch.Size([10, 1024, 74])\tnframe 1184\n","Robbery137_x264\t fea_clip.shape torch.Size([10, 1024, 137])\tnframe 2192\n","Shooting002_x264\t fea_clip.shape torch.Size([10, 1024, 75])\tnframe 1200\n","Shooting004_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Shooting007_x264\t fea_clip.shape torch.Size([10, 1024, 89])\tnframe 1424\n","Shooting008_x264\t fea_clip.shape torch.Size([10, 1024, 101])\tnframe 1616\n","Shooting010_x264\t fea_clip.shape torch.Size([10, 1024, 165])\tnframe 2640\n","Shooting011_x264\t fea_clip.shape torch.Size([10, 1024, 250])\tnframe 4000\n","Shooting013_x264\t fea_clip.shape torch.Size([10, 1024, 67])\tnframe 1072\n","Shooting015_x264\t fea_clip.shape torch.Size([10, 1024, 107])\tnframe 1712\n","Shooting018_x264\t fea_clip.shape torch.Size([10, 1024, 112])\tnframe 1792\n","Shooting019_x264\t fea_clip.shape torch.Size([10, 1024, 172])\tnframe 2752\n","Shooting021_x264\t fea_clip.shape torch.Size([10, 1024, 79])\tnframe 1264\n","Shooting022_x264\t fea_clip.shape torch.Size([10, 1024, 284])\tnframe 4544\n","Shooting024_x264\t fea_clip.shape torch.Size([10, 1024, 125])\tnframe 2000\n","Shooting026_x264\t fea_clip.shape torch.Size([10, 1024, 87])\tnframe 1392\n","Shooting028_x264\t fea_clip.shape torch.Size([10, 1024, 118])\tnframe 1888\n","Shooting032_x264\t fea_clip.shape torch.Size([10, 1024, 1355])\tnframe 21680\n","Shooting033_x264\t fea_clip.shape torch.Size([10, 1024, 226])\tnframe 3616\n","Shooting034_x264\t fea_clip.shape torch.Size([10, 1024, 88])\tnframe 1408\n","Shooting037_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Shooting043_x264\t fea_clip.shape torch.Size([10, 1024, 117])\tnframe 1872\n","Shooting046_x264\t fea_clip.shape torch.Size([10, 1024, 318])\tnframe 5088\n","Shooting047_x264\t fea_clip.shape torch.Size([10, 1024, 517])\tnframe 8272\n","Shooting048_x264\t fea_clip.shape torch.Size([10, 1024, 171])\tnframe 2736\n","Shoplifting001_x264\t fea_clip.shape torch.Size([10, 1024, 271])\tnframe 4336\n","Shoplifting004_x264\t fea_clip.shape torch.Size([10, 1024, 417])\tnframe 6672\n","Shoplifting005_x264\t fea_clip.shape torch.Size([10, 1024, 122])\tnframe 1952\n","Shoplifting007_x264\t fea_clip.shape torch.Size([10, 1024, 320])\tnframe 5120\n","Shoplifting010_x264\t fea_clip.shape torch.Size([10, 1024, 171])\tnframe 2736\n","Shoplifting015_x264\t fea_clip.shape torch.Size([10, 1024, 141])\tnframe 2256\n","Shoplifting016_x264\t fea_clip.shape torch.Size([10, 1024, 92])\tnframe 1472\n","Shoplifting017_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Shoplifting020_x264\t fea_clip.shape torch.Size([10, 1024, 360])\tnframe 5760\n","Shoplifting021_x264\t fea_clip.shape torch.Size([10, 1024, 221])\tnframe 3536\n","Shoplifting022_x264\t fea_clip.shape torch.Size([10, 1024, 136])\tnframe 2176\n","Shoplifting027_x264\t fea_clip.shape torch.Size([10, 1024, 117])\tnframe 1872\n","Shoplifting028_x264\t fea_clip.shape torch.Size([10, 1024, 84])\tnframe 1344\n","Shoplifting029_x264\t fea_clip.shape torch.Size([10, 1024, 136])\tnframe 2176\n","Shoplifting031_x264\t fea_clip.shape torch.Size([10, 1024, 32])\tnframe 512\n","Shoplifting033_x264\t fea_clip.shape torch.Size([10, 1024, 56])\tnframe 896\n","Shoplifting034_x264\t fea_clip.shape torch.Size([10, 1024, 746])\tnframe 11936\n","Shoplifting037_x264\t fea_clip.shape torch.Size([10, 1024, 86])\tnframe 1376\n","Shoplifting039_x264\t fea_clip.shape torch.Size([10, 1024, 175])\tnframe 2800\n","Shoplifting044_x264\t fea_clip.shape torch.Size([10, 1024, 909])\tnframe 14544\n","Shoplifting049_x264\t fea_clip.shape torch.Size([10, 1024, 134])\tnframe 2144\n","Stealing019_x264\t fea_clip.shape torch.Size([10, 1024, 306])\tnframe 4896\n","Stealing036_x264\t fea_clip.shape torch.Size([10, 1024, 156])\tnframe 2496\n","Stealing058_x264\t fea_clip.shape torch.Size([10, 1024, 311])\tnframe 4976\n","Stealing062_x264\t fea_clip.shape torch.Size([10, 1024, 97])\tnframe 1552\n","Stealing079_x264\t fea_clip.shape torch.Size([10, 1024, 365])\tnframe 5840\n","Vandalism007_x264\t fea_clip.shape torch.Size([10, 1024, 71])\tnframe 1136\n","Vandalism015_x264\t fea_clip.shape torch.Size([10, 1024, 186])\tnframe 2976\n","Vandalism017_x264\t fea_clip.shape torch.Size([10, 1024, 63])\tnframe 1008\n","Vandalism028_x264\t fea_clip.shape torch.Size([10, 1024, 280])\tnframe 4480\n","Vandalism036_x264\t fea_clip.shape torch.Size([10, 1024, 90])\tnframe 1440\n","groundtruths.shape (1113424,)\n","saved to ./groundtruths.json\n"]}]},{"cell_type":"code","source":["with open('./groundtruths.json', 'rb') as fp:\n","    groundtruths = json.load(fp)\n","    print(\"groundtruths length:\", len(groundtruths))"],"metadata":{"id":"z66fGsdj9Cti","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666927541169,"user_tz":-480,"elapsed":8,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"91644fff-a16a-448b-b2c0-1cfddf0666a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["groundtruths length: 1113424\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjLZtBt0vDaE"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["---\n","# demo"],"metadata":{"id":"CXEb40JEd4Rt"}},{"cell_type":"code","source":["video_dir=\"./dataset/Abuse/\"\n","\n","#Orignal size (for train data only)\n","run(video_dir=video_dir, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", overwrite=True, category=\"ori\", randCrop_version=0)\n","#10-crop (for train/test data )\n","for i in range(10):\n","    run(video_dir=video_dir, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", overwrite=True, category=\"10-crop\", randCrop_version=i)\n","#random-crop (for train data only)\n","#random 7version\n","for i in range(7):\n","    run(video_dir=video_dir, save_dir = \"./feature_extracted/\", anomaly_train_txt = \"./dataset/Anomaly_Train.txt\", overwrite=True, category=\"random-crop\", randCrop_version=i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiZAeJv3DbBo","executionInfo":{"status":"ok","timestamp":1667289598365,"user_tz":-480,"elapsed":112138,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"43816f12-cc02-4cff-d85d-5d9608b89c97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=========== ori version 0 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\t"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["segments_spatial_fea: torch.Size([1024, 32, 8, 10])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/ori_spatial/Abuse002_x264.pt\tsave path: ./feature_extracted/train/ori/Abuse002_x264.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== 10-crop version 0 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version0.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version0.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version0.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version0.pt\n","=========== 10-crop version 1 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version1.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version1.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version1.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version1.pt\n","=========== 10-crop version 2 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version2.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version2.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version2.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version2.pt\n","=========== 10-crop version 3 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version3.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version3.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version3.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version3.pt\n","=========== 10-crop version 4 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version4.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version4.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version4.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version4.pt\n","=========== 10-crop version 5 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version5.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version5.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version5.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version5.pt\n","=========== 10-crop version 6 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version6.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version6.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version6.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version6.pt\n","=========== 10-crop version 7 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version7.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version7.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version7.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version7.pt\n","=========== 10-crop version 8 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version8.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version8.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version8.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version8.pt\n","=========== 10-crop version 9 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/10-crop_spatial/Abuse002_x264_version9.pt\tsave path: ./feature_extracted/train/10-crop/Abuse002_x264_version9.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tclips_spatial_fea: torch.Size([1024, 88, 7, 9])\t\tclips_pooling_fea: torch.Size([1024, 88])\tspatial save path: ./feature_extracted/test/10-crop_spatial/Abuse028_x264_version9.pt\tsave path: ./feature_extracted/test/10-crop/Abuse028_x264_version9.pt\n","=========== random-crop version 0 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version0.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version0.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== random-crop version 1 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version1.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version1.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== random-crop version 2 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version2.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version2.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== random-crop version 3 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version3.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version3.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== random-crop version 4 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version4.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version4.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== random-crop version 5 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version5.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version5.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n","=========== random-crop version 6 =================\n","[1/2] Abuse002_x264.mp4\tanomaly train\tsegments_spatial_fea: torch.Size([1024, 32, 7, 9])\tsegments_pooling_fea: torch.Size([1024, 32])\tspatial save path: ./feature_extracted/train/random-crop_spatial/Abuse002_x264_version6.pt\tsave path: ./feature_extracted/train/random-crop/Abuse002_x264_version6.pt\n","[2/2] Abuse028_x264.mp4\tanomaly test\tis test data, skip\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"6d26fa0f97be58a73aca1854eda93fb3249a95ecb978fc1b1600222def8af89e"}},"colab":{"provenance":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}