{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNw54GjbxbbW1EYUpYfRmEF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_efZj5qYVEq","executionInfo":{"status":"ok","timestamp":1683093870428,"user_tz":-480,"elapsed":27999,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"976fd092-1e20-4ee7-d471-1bfd8a213431"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["cd \"/content/gdrive/My Drive/fyp1\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5nx7m6QwYXNR","executionInfo":{"status":"ok","timestamp":1683093870428,"user_tz":-480,"elapsed":10,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"881c6dd7-22ae-44b3-d90d-9854cc6562f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/fyp1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Np4Zs52UvpHr","executionInfo":{"status":"ok","timestamp":1683093887834,"user_tz":-480,"elapsed":17412,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"0e879a14-4cc6-4896-9ec7-243a7937110c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gradio\n","  Downloading gradio-3.28.1-py3-none-any.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n","Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n","Collecting gradio-client>=0.1.3\n","  Downloading gradio_client-0.1.4-py3-none-any.whl (286 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/286.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn\n","  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi\n","  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx\n","  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting python-multipart\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n","Collecting orjson\n","  Downloading orjson-3.8.11-cp310-cp310-manylinux_2_28_x86_64.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.0\n","  Downloading websockets-11.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n","Collecting mdit-py-plugins<=0.3.3\n","  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles\n","  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n","Collecting ffmpy\n","  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.13.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n","Collecting semantic-version\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.1.3->gradio) (2023.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.1.3->gradio) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.12.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n","Collecting linkify-it-py<3,>=1\n","  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting starlette<0.27.0,>=0.26.1\n","  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2022.12.7)\n","Collecting httpcore<0.18.0,>=0.15.0\n","  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.39.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.0.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.15)\n","Collecting h11>=0.8\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->gradio) (8.1.3)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n","Collecting uc-micro-py\n","  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=f82e3dd5c23702da3aaa5f9fdcf9b37fac41892797aa52bbe6b98bd2e9556fde\n","  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, multidict, h11, frozenlist, async-timeout, aiofiles, yarl, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, aiosignal, httpx, fastapi, aiohttp, gradio-client, gradio\n","Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 fastapi-0.95.1 ffmpy-0.3.0 frozenlist-1.3.3 gradio-3.28.1 gradio-client-0.1.4 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.14.1 linkify-it-py-2.0.2 mdit-py-plugins-0.3.3 multidict-6.0.4 orjson-3.8.11 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.26.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.2 yarl-1.9.2\n"]}],"source":["! pip install gradio"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from PIL import Image"],"metadata":{"id":"gxke4R3_8m5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MaxPool3dSamePadding(nn.MaxPool3d):\n","    \n","    def compute_pad(self, dim, s):\n","        if s % self.stride[dim] == 0:\n","            return max(self.kernel_size[dim] - self.stride[dim], 0)\n","        else:\n","            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n","\n","    def forward(self, x):\n","        # compute 'same' padding\n","        (batch, channel, t, h, w) = x.size()\n","        out_t = np.ceil(float(t) / float(self.stride[0]))\n","        out_h = np.ceil(float(h) / float(self.stride[1]))\n","        out_w = np.ceil(float(w) / float(self.stride[2]))\n","        pad_t = self.compute_pad(0, t)\n","        pad_h = self.compute_pad(1, h)\n","        pad_w = self.compute_pad(2, w)\n","\n","        pad_t_f = pad_t // 2\n","        pad_t_b = pad_t - pad_t_f\n","        pad_h_f = pad_h // 2\n","        pad_h_b = pad_h - pad_h_f\n","        pad_w_f = pad_w // 2\n","        pad_w_b = pad_w - pad_w_f\n","\n","        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n","        x = F.pad(x, pad)\n","\n","        return super().forward(x)\n","    \n","class Unit3D(nn.Module):\n","\n","    def __init__(self, in_channels,\n","                 output_channels,\n","                 kernel_shape=(1, 1, 1),\n","                 stride=(1, 1, 1),\n","                 padding=0,\n","                 activation_fn=F.relu,\n","                 use_batch_norm=True,\n","                 use_bias=False,\n","                 name='unit_3d'):\n","        \n","        \"\"\"Initializes Unit3D module.\"\"\"\n","        super(Unit3D, self).__init__()\n","        \n","        self._output_channels = output_channels\n","        self._kernel_shape = kernel_shape\n","        self._stride = stride\n","        self._use_batch_norm = use_batch_norm\n","        self._activation_fn = activation_fn\n","        self._use_bias = use_bias\n","        self.name = name\n","        self.padding = padding\n","        \n","        self.conv3d = nn.Conv3d(in_channels=in_channels,\n","                                out_channels=self._output_channels,\n","                                kernel_size=self._kernel_shape,\n","                                stride=self._stride,\n","                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n","                                bias=self._use_bias)\n","        \n","        if self._use_batch_norm:\n","            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n","\n","    def compute_pad(self, dim, s):\n","        if s % self._stride[dim] == 0:\n","            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n","        else:\n","            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n","\n","            \n","    def forward(self, x):\n","        # compute 'same' padding\n","        (batch, channel, t, h, w) = x.size()\n","        out_t = np.ceil(float(t) / float(self._stride[0]))\n","        out_h = np.ceil(float(h) / float(self._stride[1]))\n","        out_w = np.ceil(float(w) / float(self._stride[2]))\n","        pad_t = self.compute_pad(0, t)\n","        pad_h = self.compute_pad(1, h)\n","        pad_w = self.compute_pad(2, w)\n","\n","        pad_t_f = pad_t // 2\n","        pad_t_b = pad_t - pad_t_f\n","        pad_h_f = pad_h // 2\n","        pad_h_b = pad_h - pad_h_f\n","        pad_w_f = pad_w // 2\n","        pad_w_b = pad_w - pad_w_f\n","\n","        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n","        x = F.pad(x, pad)   \n","\n","        x = self.conv3d(x)\n","        if self._use_batch_norm:\n","            x = self.bn(x)\n","        if self._activation_fn is not None:\n","            x = self._activation_fn(x)\n","            \n","        return x\n","\n","class InceptionModule(nn.Module):\n","    def __init__(self, in_channels, out_channels, name):\n","        super(InceptionModule, self).__init__()\n","\n","        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n","                         name=name+'/Branch_0/Conv3d_0a_1x1')\n","        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n","                          name=name+'/Branch_1/Conv3d_0a_1x1')\n","        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n","                          name=name+'/Branch_1/Conv3d_0b_3x3')\n","        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n","                          name=name+'/Branch_2/Conv3d_0a_1x1')\n","        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n","                          name=name+'/Branch_2/Conv3d_0b_3x3')\n","        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n","                                stride=(1, 1, 1), padding=0)\n","        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n","                          name=name+'/Branch_3/Conv3d_0b_1x1')\n","        self.name = name\n","\n","    def forward(self, x):    \n","        b0 = self.b0(x)\n","        b1 = self.b1b(self.b1a(x))\n","        b2 = self.b2b(self.b2a(x))\n","        b3 = self.b3b(self.b3a(x))\n","        return torch.cat([b0,b1,b2,b3], dim=1)\n","\n","class InceptionI3d(nn.Module):\n","    \"\"\"Inception-v1 I3D architecture.\n","    The model is introduced in:\n","        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n","        Joao Carreira, Andrew Zisserman\n","        https://arxiv.org/pdf/1705.07750v1.pdf.\n","    See also the Inception architecture, introduced in:\n","        Going deeper with convolutions\n","        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n","        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n","        http://arxiv.org/pdf/1409.4842v1.pdf.\n","    \"\"\"\n","\n","    # Endpoints of the model in order. During construction, all the endpoints up\n","    # to a designated `final_endpoint` are returned in a dictionary as the\n","    # second return value.\n","    VALID_ENDPOINTS = (\n","        'Conv3d_1a_7x7',\n","        'MaxPool3d_2a_3x3',\n","        'Conv3d_2b_1x1',\n","        'Conv3d_2c_3x3',\n","        'MaxPool3d_3a_3x3',\n","        'Mixed_3b',\n","        'Mixed_3c',\n","        'MaxPool3d_4a_3x3',\n","        'Mixed_4b',\n","        'Mixed_4c',\n","        'Mixed_4d',\n","        'Mixed_4e',\n","        'Mixed_4f',\n","        'MaxPool3d_5a_2x2',\n","        'Mixed_5b',\n","        'Mixed_5c',\n","        'Logits',\n","        'Predictions',\n","    )\n","\n","    def __init__(self, num_classes=400, spatial_squeeze=True,\n","                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n","        \"\"\"Initializes I3D model instance.\n","        Args:\n","          num_classes: The number of outputs in the logit layer (default 400, which\n","              matches the Kinetics dataset).\n","          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n","              before returning (default True).\n","          final_endpoint: The model contains many possible endpoints.\n","              `final_endpoint` specifies the last endpoint for the model to be built\n","              up to. In addition to the output at `final_endpoint`, all the outputs\n","              at endpoints up to `final_endpoint` will also be returned, in a\n","              dictionary. `final_endpoint` must be one of\n","              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n","          name: A string (optional). The name of this module.\n","        Raises:\n","          ValueError: if `final_endpoint` is not recognized.\n","        \"\"\"\n","\n","        if final_endpoint not in self.VALID_ENDPOINTS:\n","            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n","\n","        super(InceptionI3d, self).__init__()\n","        self._num_classes = num_classes\n","        self._spatial_squeeze = spatial_squeeze\n","        self._final_endpoint = final_endpoint\n","        self.logits = None\n","\n","        if self._final_endpoint not in self.VALID_ENDPOINTS:\n","            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n","\n","        self.end_points = {}\n","        end_point = 'Conv3d_1a_7x7'\n","        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n","                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'MaxPool3d_2a_3x3'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'Conv3d_2b_1x1'\n","        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n","                                       name=name+end_point)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'Conv3d_2c_3x3'\n","        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n","                                       name=name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'MaxPool3d_3a_3x3'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","        \n","        end_point = 'Mixed_3b'\n","        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_3c'\n","        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'MaxPool3d_4a_3x3'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4b'\n","        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4c'\n","        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4d'\n","        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4e'\n","        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_4f'\n","        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'MaxPool3d_5a_2x2'\n","        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n","                                                             padding=0)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_5b'\n","        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Mixed_5c'\n","        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n","        if self._final_endpoint == end_point: return\n","\n","        end_point = 'Logits'\n","        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n","                                     stride=(1, 1, 1))\n","        self.dropout = nn.Dropout(dropout_keep_prob)\n","        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n","                             kernel_shape=[1, 1, 1],\n","                             padding=0,\n","                             activation_fn=None,\n","                             use_batch_norm=False,\n","                             use_bias=True,\n","                             name='logits')\n","\n","        self.build()\n","\n","\n","    def replace_logits(self, num_classes):\n","        self._num_classes = num_classes\n","        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n","                             kernel_shape=[1, 1, 1],\n","                             padding=0,\n","                             activation_fn=None,\n","                             use_batch_norm=False,\n","                             use_bias=True,\n","                             name='logits')\n","        \n","    def build(self):\n","        for k in self.end_points.keys():\n","            self.add_module(k, self.end_points[k])\n","        \n","    def forward(self, x):\n","        print(\"inside forward\")\n","        for end_point in self.VALID_ENDPOINTS:\n","            if end_point in self.end_points:\n","                print(f\"forwarding {end_point}\")\n","                x = self._modules[end_point](x) # use _modules to work with dataparallel\n","        print(f\"before logit shape {x.shape}\")\n","        x = self.logits(self.dropout(self.avg_pool(x)))\n","        print(f\"after logit shape {x.shape}\")\n","        if self._spatial_squeeze:\n","            logits = x.squeeze(3).squeeze(3)\n","            print(f\"after squeeeze shape {logits.shape}\")\n","        # logits is batch X time X classes, which is what we want to work with\n","        return logits\n","\n","    def extract_features(self, x, myEndPoint):\n","        #x (original size of frame) = (b,c,nframe,h,w):(1,3,16,240,320)\n","        #x_cropped                  = (b,c,nframe,h,w):(1,3,16,210,280)\n","\n","        #I3D feature extraction\n","        for end_point in self.VALID_ENDPOINTS:\n","            if end_point in self.end_points:\n","                x = self._modules[end_point](x)\n","            if end_point == myEndPoint:\n","                break\n","        #====================================\n","        # x         => (b,c,t,h,w):(1,1024,2,8,10)\n","        # x_cropped => (b,c,t,h,w):(1,1024,2,7,9)\n","        x = torch.norm(x, dim=2) #l2 normalization for t dimension\n","        # x         => (b,c,t,h,w):(1,1024,1,8,10)\n","        # x_cropped => (b,c,t,h,w):(1,1024,1,7,9)\n","        spatial_fea = torch.squeeze(x) \n","        # spatial_fea         => (c,h,w):(1024,8,10)\n","        # spatial_fea_cropped => (c,h,w):(1024,7,9)\n","\n","        x = F.adaptive_avg_pool2d(x, (1, 1)) #global pooling\n","        # x         => (b,c,t,h,w):(1,1024,1,1,1)\n","        # x_cropped => (b,c,t,h,w):(1,1024,1,1,1)\n","        x = torch.squeeze(x) #sequeeze batch(for ori/random-crop data) and spatial dimension\n","        # x         => (c,):(1024,)\n","        # x_cropped => (c,):(1024,)\n","        pooling_fea = x\n","        \n","        return spatial_fea, pooling_fea\n","        #============================================\n","        # spatial_fea         => (c,h,w):(1024,8,10)\n","        # spatial_fea_cropped => (c,h,w):(1024,7,9)\n","        # pooling_fea         => (c,):(1024,)\n","        # pooling_fea_cropped => (c,):(1024,)\n","        #============================================\n","\n","def custom_I3D(mode='rgb', load_model='./rgb_imagenet.pt'):\n","    # setup the model\n","    if mode == 'flow':\n","        i3d = InceptionI3d(400, in_channels=2)\n","    else:\n","        i3d = InceptionI3d(400, in_channels=3)\n","    i3d.load_state_dict(torch.load(load_model))\n","    return i3d\n","def feature_extract(i3d, x, myEndPoint):\n","    #get available device\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    #pass to device available\n","    i3d = i3d.to(device)\n","    x = x.to(device)\n","\n","    i3d.eval()  # Set model to evaluate mode\n","    #x (original size of frame) = (b,c,nframe,h,w):(1,3,16,240,320)\n","    #x_cropped                  = (b,c,nframe,h,w):(1,3,16,210,280)\n","    with torch.no_grad():\n","        spatial_fea, pooling_fea = i3d.extract_features(x, myEndPoint)\n","    \n","    return spatial_fea, pooling_fea\n","    #============================================\n","    # spatial_fea         => (c,h,w):(1024,8,10)\n","    # spatial_fea_cropped => (c,h,w):(1024,7,9)\n","    # pooling_fea         => (c,):(1024,)\n","    # pooling_fea_cropped => (c,):(1024,)\n","    #============================================\n","transform = transforms.Compose([\n","    #transforms.Resize((240,320)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n"],"metadata":{"id":"JQvDJdYI8aZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Contrastive Unet model"],"metadata":{"id":"gkgvpxaN_Ub3"}},{"cell_type":"code","source":["class Contrastive_head(nn.Module):\n","    def __init__(self, num_center=2, in_channels=32, lambda_1=0.0001, beta=0.0001, m=1.25):\n","        super().__init__()\n","        \n","        self.num_class = 2\n","        self.num_center = num_center #number of center per class\n","        self.in_channels = in_channels\n","        self.lambda_1 = lambda_1\n","        self.beta = beta\n","        self.m = torch.tensor(m)\n","        self.Centers = nn.Parameter(torch.randn(2, num_center, self.in_channels), requires_grad=True) #(nclass,ncenter,c)\n","\n","    def forward(self, x, device):\n","        #x = (b*3,c,k)\n","        #top 3 score from normal video\n","        #last 3 score from anomaly video\n","        #top 3 score from anomaly video\n","        \n","        # compute intra distance\n","        intra_distance = torch.mean(self.compute_intra_dist(x[:int(x.shape[0]/3)], self.Centers[0]) + \\\n","                                    self.compute_intra_dist(x[int(x.shape[0]/3):int(x.shape[0]/3*2)], self.Centers[0]) + \\\n","                                    self.compute_intra_dist(x[int(x.shape[0]/3*2):], self.Centers[1]))\n","        \n","        # compute inter distance\n","        inter_distance = self.compute_inter_dist(self.Centers[0], self.Centers[1], self.m, device) \n","\n","        # compute multicenter loss\n","        mc_loss = self.lambda_1 * intra_distance + self.beta * inter_distance\n","\n","        return mc_loss\n","\n","    def compute_intra_dist(self, x, centers):\n","        #x = (b,c,nsegment)\n","        centers2 = centers.repeat(x.shape[0], x.shape[2], 1, 1)\n","        #centers2 = (b, nsegment, ncenter, c)\n","        centers2 = centers2.permute(2,0,3,1)\n","        #centers2 = (ncenter,b,c,nsegment)\n","        dist_ctr_fea = (centers2 - x).square().sum(dim=2).sqrt()\n","        #dist_ctr_fea = (ncenter, b, nsegment)    \n","        min_dist_fea_ctr, idx = torch.min(dist_ctr_fea, dim=0)\n","        #min_dist_fea_ctr = (b, nsegment)\n","        sum_dist_fea_ctr = torch.mean(min_dist_fea_ctr, dim=1)\n","        #sum_dist_fea_ctr = (b)\n","        return sum_dist_fea_ctr\n","\n","    def compute_inter_dist(self, norm_centers, anom_centers, m, device):\n","        #norm_centers = (ncenter, c)\n","        #anom_centers = (ncenter, c)\n","        total_ctr = norm_centers.shape[0] + anom_centers.shape[0] # (ncenter+ncenter)\n","        centers = torch.cat((norm_centers, anom_centers), dim=0)\n","        #centers = (ncenter+ncenter, c)\n","        centers2 = centers.unsqueeze(0)\n","        #centers = (1, ncenter, c)\n","        dist = torch.cdist(centers2, centers2).squeeze()\n","        #dist = (total_ctr,total_ctr)\n","        margin = m.repeat(dist.shape[0], dist.shape[1]) #margin2 = (total_ctr,total_ctr)\n","        margin = margin.to(device)\n","        zeros = torch.zeros_like(dist) #zeros = (total_ctr,total_ctr)\n","        zeros = zeros.to(device)\n","        inter_dist = torch.max(zeros, margin-dist).sum() / (total_ctr*(total_ctr-1)) \n","        return inter_dist\n","\n","# Reference: https://amaarora.github.io/2020/09/13/unet.html\n","\n","class Block(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        \n","        super().__init__()\n","\n","        self.in_ch = in_ch\n","        self.out_ch = out_ch        \n","        \n","        self.conv1 = nn.Conv1d (in_ch, out_ch, kernel_size=5, stride=1, padding=2)\n","        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=5, stride=1, padding=2)        \n","        self.conv3 = nn.Conv1d(out_ch, out_ch, kernel_size=5, stride=1, padding=2)\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        r = x\n","        r = self.conv2(r)\n","        r = F.relu(r)\n","        r = self.conv3(r)\n","        x = x + r\n","        x = F.relu(x)\n","        return x\n","        \n","class Encoder(nn.Module):\n","    def __init__(self, in_channels, blk_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.blk_channels = blk_channels\n","        channels = [in_channels] + list(blk_channels)\n","        self.enc_blocks = nn.ModuleList([Block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n","    def forward(self, x):\n","        enc_out = []\n","        for i, block in enumerate(self.enc_blocks):\n","            x = block(x)\n","            enc_out.append(x)\n","            if i < len(self.enc_blocks)-1:\n","                x = F.max_pool1d(x, 2, 2)\n","        return enc_out\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, blk_channels):\n","        super().__init__()\n","        self.channels = blk_channels[::-1]  # reverse the blk_channels\n","        self.upconvs = nn.ModuleList ([nn.ConvTranspose1d(self.channels[i], self.channels[i+1], kernel_size=2, stride=2) \n","                                       for i in range(len(self.channels)-1)])\n","        self.dec_blocks = nn.ModuleList([Block(self.channels[i]*2, self.channels[i+1]) \n","                                         for i in range(len(self.channels)-1)]) \n","        \n","    def crop(self, enc_fea, x):\n","        _, _, L = x.shape\n","        _, _, Le = enc_fea.shape\n","        start = (Le - L)//2\n","        return enc_fea[:,:, start:start+L]\n","        \n","    def forward(self, enc_out, inputs):\n","        enc_out = enc_out[::-1]  # reverse the order\n","        x = enc_out[0]\n","        enc_out = enc_out[1:]\n","        for i in range(len(self.channels)-1):\n","            x1 = self.upconvs[i](x)\n","            x2 = self.crop(enc_out[i], x1)\n","            x = torch.cat([x1, x2], dim=1)\n","            x = self.dec_blocks[i](x)\n","        x = F.interpolate(x, inputs.shape[-1])\n","        return x\n","    \n","class Head(nn.Module):\n","    def __init__ (self, channels):\n","        super().__init__()\n","        self.layers = nn.ModuleList([nn.Conv1d(channels[i], channels[i+1], kernel_size=1) for i in range(len(channels)-1)])\n","        self.dropout = nn.Dropout(p=0.7) \n","        \n","    def forward(self, x, isTraining):\n","        for i, layer in enumerate(self.layers):\n","            x = layer(x)\n","            if i == len(self.layers) - 1:\n","                x = torch.sigmoid(x)\n","            else:\n","                x = F.relu(x)\n","                if isTraining == True:\n","                    x = self.dropout(x)\n","                last_fea = x\n","        return last_fea, x\n","        \n","class Model(nn.Module):\n","    def __init__(self, in_channels=1024, \n","                 blk_channels  = (1024, 1024, 1024, 1024),  \n","                 head_channels = (1024, 512, 32, 1)):\n","        super().__init__()\n","        self.in_channels  = in_channels\n","        self.blk_channels = blk_channels\n","        self.encoder = Encoder(in_channels, blk_channels)\n","        self.decoder = Decoder(blk_channels)\n","        self.head    = Head(head_channels)\n","        self.contrastive_head = Contrastive_head(num_center=16, in_channels=32, lambda_1=0.00224, beta=0.2304, m=1.25)\n","\n","    def forward(self, inputs, isTraining=False, device=\"cpu\"):\n","        #inputs = (b*2,c,nsegment):(?,1024,32)\n","        enc_output = self.encoder(inputs)               \n","        dec_output = self.decoder(enc_output, inputs)      \n","        last_fea, segment_scores = self.head(dec_output, isTraining)\n","        #last_fea = (b*2, 32, nsegment), features just before classification layer\n","        #segment_scores = (b*2, 1, nsegment)\n","        if isTraining == True:\n","            \"\"\"#============= loss function ==============\"\"\"\n","            bce, idx_topK_anom, idx_lastK_anom, idx_topK_norm = self.myLossFunction(segment_scores)\n","            #idx_topK_anom = (b,ksegment)\n","\n","            \"\"\"#============== get topk & lastk feature ====================\"\"\"\n","            last_fea = last_fea.to(\"cpu\")\n","            last_fea = last_fea.permute(1,0,2) #convert (b*2, 32, nsegment) to (32, b*2, nsegment)\n","            normal_features = last_fea[:,:int(last_fea.shape[1]/2)] #segment feature in normal video \n","            anomaly_features = last_fea[:,int(last_fea.shape[1]/2):] #segment feature in anomaly video \n","            #normal_features = (32, b, nsegment)\n","            #anomaly_features = (32, b, nsegment)\n","\n","            total_topk_norm_feature = torch.zeros(0)\n","            total_topk_abn_feature = torch.zeros(0)\n","            total_lastk_abn_feature = torch.zeros(0)\n","            #get topk features from normal video\n","            for normal_feature in normal_features: \n","                topk_norm_feature = torch.gather(normal_feature, 1, idx_topK_norm)     \n","                #topk_norm_feature = (b,k)\n","                topk_norm_feature = torch.unsqueeze(topk_norm_feature, 0)\n","                #topk_norm_feature = (1,b,k)\n","                total_topk_norm_feature = torch.cat((total_topk_norm_feature, topk_norm_feature), dim=0)  \n","\n","            #get topk and lastk features from anomaly video\n","            for abnormal_feature in anomaly_features: \n","                topk_abn_feature = torch.gather(abnormal_feature, 1, idx_topK_anom)  \n","                lastk_abn_feature = torch.gather(abnormal_feature, 1, idx_lastK_anom)   \n","                #topk_abn_feature = (b,k)\n","                topk_abn_feature = torch.unsqueeze(topk_abn_feature, 0)\n","                lastk_abn_feature = torch.unsqueeze(lastk_abn_feature, 0)\n","                #topk_abn_feature = (1,b,k)\n","                total_topk_abn_feature = torch.cat((total_topk_abn_feature, topk_abn_feature), dim=0)  \n","                total_lastk_abn_feature = torch.cat((total_lastk_abn_feature, lastk_abn_feature), dim=0) \n","            \n","            #total_topk_norm_feature = (c,b,k)\n","            #total_topk_abn_feature = (c,b,k)\n","            #total_lastk_abn_feature = (c,b,k)\n","            selected_feature = torch.cat((total_topk_norm_feature, total_lastk_abn_feature, total_topk_abn_feature), dim=1) #first half is lastK features, second half is topK features \n","            selected_feature = selected_feature.permute(1,0,2) #convert (c,b*3,k) to (b*3,c,k)\n","            selected_feature = selected_feature.to(device)\n","            \"\"\"#============== contrastive regularization =====================\"\"\" \n","            mc_loss = self.contrastive_head(selected_feature, device)\n","            combination_loss = bce + mc_loss\n","            return combination_loss\n","        else:\n","            return last_fea, segment_scores \n","            #last_fea = (b*2, c, nsegment)\n","            #segment_scores=(b*2, 1, nsegment)\n","\n","    def myLossFunction(self, segment_scores, k=3, lambda_1=0.00008, lambda_2=0.00008):\n","        #convert (b,1,nsegment) to (b,nsegment)\n","        segment_scores = torch.squeeze(segment_scores, 1) \n","\n","        #to cpu\n","        segment_scores = segment_scores.to(\"cpu\")\n","\n","        #segment_scores = (b*2, nsegment)\n","        normal_scores = segment_scores[:int(segment_scores.shape[0]/2)]\n","        #normal_scores (b,nsegment)\n","\n","        anomaly_scores = segment_scores[int(segment_scores.shape[0]/2):]\n","        #anomaly_scores (b,nsegment)\n","        #========== get top-k segment score ============\n","        topk_norm_scores, idx_topK_norm = torch.topk(normal_scores, k=k, dim=1, largest=True)   #top-k normal score (b,k)\n","        topk_anom_scores, idx_topK_anom = torch.topk(anomaly_scores, k=k, dim=1, largest=True) #top-k anomaly score (b,k)\n","        _, idx_lastK_anom = torch.topk(anomaly_scores, k=k, dim=1, largest=False) #last-k anomaly score (b,k)\n","        topk_scores = torch.cat((topk_norm_scores, topk_anom_scores), dim=0) #(b*2,k)\n","\n","        #initialize label\n","        zeros = torch.zeros_like(topk_norm_scores) #(b,k)\n","        ones = torch.ones_like(topk_anom_scores) #(b,k)\n","        label = torch.cat((zeros, ones), dim=0) #(b*2,k)\n","\n","        #=========== binary cross entropy loss ====================\n","        bce = F.binary_cross_entropy(topk_scores, label)  \n","\n","        anomaly_scores = anomaly_scores.permute(1,0) #(nsegment,b)\n","        normal_scores = normal_scores.permute(1,0) #(nsegment,b)\n","        \n","        #========= get temporal smoothness ==========\n","        diff_neighbour = torch.randn(anomaly_scores.shape[0]-1, anomaly_scores.shape[1]) # (nsegment-1, b) difference of 2 neighbour segment\n","        for i in range(anomaly_scores.shape[0]-1): # #segments-1\n","            diff_neighbour[i] = torch.pow((anomaly_scores[i] - anomaly_scores[i+1]), 2) \n","\n","        temp_smoothness = lambda_1 * torch.sum(diff_neighbour, dim=0, keepdim=True) \n","\n","        #========= get sparsity ==========\n","        sparsity = lambda_2 * torch.sum(anomaly_scores, dim=0, keepdim=True) #(1, b)\n","\n","        #final cost \n","        cost = bce + torch.mean(temp_smoothness) + torch.mean(sparsity) \n","\n","        return cost, idx_topK_anom, idx_lastK_anom, idx_topK_norm"],"metadata":{"id":"zjkg1tic_Tj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import os\n","import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","def fea_extract(video, i3d, randCrop_version):\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    anomaly_scores = []\n","    tenCrop_loc = [[0,0], [0,40], [30,0], [30,40], [15,20],\n","                [0,0], [0,40], [30,0], [30,40], [15,20]]\n","    height_top = tenCrop_loc[randCrop_version][0]\n","    height_bottom = height_top + 210\n","    width_left = tenCrop_loc[randCrop_version][1]\n","    width_right = width_left + 280\n","\n","    if randCrop_version <= 4:\n","        random_flip = False\n","    else: #for version 5~9\n","        random_flip = True\n","\n","    f = [] #store 16-frame / 1 clip. reset to empty after feature extracted\n","    total_frame = 0 #total number of frame\n","    curr_nclip = 0   #current number of clip\n","    max_nclip = 21000 #WE ASSUME THE MAXIMUM #CLIP FOR A VIDEO IS 21000 CLIPS (enough for 3 hour long video) approximately 6GB\n","    isFirstClip = True\n","    \n","    #============== create tensor for transformed 16-frame ============\n","    sixteen_frames = torch.randn(16,3,210,280) #store tensor that transformed from PIL image. (nframe,c,h,w):(16,3,210,280)\n","\n","    #============== load video ===============\n","    cap = cv.VideoCapture(video)\n","    #if video cannot open\n","    if (cap.isOpened()== False):\n","        raise Exception(\"Error opening video file\")\n","\n","    while(cap.isOpened()):\n","        # Capture frame-by-frame\n","        ret, frame = cap.read()\n","\n","        #if video not end yet OR total_frame less than 512. (minimum nframes = 32 segment * 16 frames ==> 512)\n","        if ret == True or total_frame < 512:\n","\n","            #if video end but total_frame less than 512\n","            if ret == False and total_frame < 512:\n","                frame = np.zeros((240, 320 , 3), dtype=np.uint8) #create black blank frame\n","\n","            total_frame += 1 \n","\n","            #================ crop frame ================\n","            frame = frame[height_top:height_bottom, width_left:width_right]\n","            #================ flip frame ================\n","            if random_flip == True:\n","                frame = cv.flip(frame, 1) #flip horizontally\n","\n","            #rescale pixel value from 0~255 to 0~1\n","            #frame = frame/255\n","\n","            #convert to PIL image\n","            frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n","            frame = Image.fromarray(frame)\n","\n","            #store current frame into a clip\n","            f.append(frame)\n","\n","            #================ for each 16-frames, extract I3D feature ================\n","            if len(f) == 16:\n","                for i, frame in enumerate(f):\n","                    #================ transformation ================\n","                    frame = transform(frame)\n","                    sixteen_frames[i] = frame  #torch.cat()  is time consuming !!! \n","                    \n","                frames = torch.transpose(sixteen_frames, 0, 1) #convert (nframe,c,h,w) to (c,nframe,h,w)\n","                frames = torch.unsqueeze(frames, 0) #add batch dimension --> frames = (b,c,nframe,h,w)\n","                frames = frames.to(device) #pass to device available\n","\n","                #================ extract I3D feature ================\n","                with torch.no_grad():\n","                    spatial_fea, pooling_fea = feature_extract(i3d, frames, myEndPoint=\"Mixed_5c\") \n","                    pooling_fea = pooling_fea.to(\"cpu\")\n","                    # pooling_fea (ori/random-crop) => feature with summarized spatial dimension (c,):(1024,)\n","                    if torch.any(torch.isnan(spatial_fea)).item() == True or torch.any(torch.isnan(pooling_fea)).item() == True:\n","                        raise Exception(\"[!] nan value exist after forward to model\")\n","                    if isFirstClip == True:\n","                        clips_pooling_fea = torch.randn((max_nclip, 1024)) #clips_pooling_fea=(nclip,c):(?, 1024)\n","                        isFirstClip = False\n","                    clips_pooling_fea[curr_nclip] = pooling_fea.clone().detach()   #torch.cat() is time consuming\n","\n","                curr_nclip += 1\n","\n","                if curr_nclip >= max_nclip:\n","                    raise Exception(\"[!] total number of clips exceed defined maximum number of clips\")\n","\n","                f = [] #reset list that store 16-frames\n","        #if video end AND total_frame more than 512\n","        else:\n","            if total_frame//16 != curr_nclip:\n","                raise Exception(\"[!] total_frame//16 != curr_nclip. total_frame\", total_frame, \"curr_nclip\", curr_nclip)\n","            if curr_nclip < 32:\n","                raise Exception(\"[!] Required at least 32 clips but only having\", curr_nclip)\n","            clips_pooling_fea = clips_pooling_fea[:curr_nclip, :].clone().detach() #remove unwanted memory \n","            cap.release()\n","            return clips_pooling_fea #clips_pooling_fea=(nclip,c):(?, 1024)\n","\n","def predict_10crop(video):\n","    \n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    #================= setting =============================\n","    saved_weight = \"./saved_model/top3pos_last3pos_top3neg_16centers_L0.00256_m1.25_b0.2304_1024c_B64/highestAUC.pt\" #85.24\n","    #=======================================================\n","    model = Model()\n","    model.load_state_dict(torch.load(saved_weight, map_location='cuda:0'))\n","    model.to(device)\n","    model.eval()\n","    i3d = custom_I3D()\n","    i3d.to(device)\n","    i3d.eval()\n","    #========================================================\n","\n","    for i in range(10):\n","        x = fea_extract(video, i3d, randCrop_version=i)\n","        #x = (nclip,c):(?, 1024)\n","        if i == 0:\n","            tenCrop_fea = torch.randn((10,1024, x.shape[0]), device=device)\n","        tenCrop_fea[i] = x.transpose(0,1)\n","        #tenCrop_fea = (ncrop,c,nclip):(10,1024,?)\n","    #=============== predict =============\n","    with torch.no_grad():\n","        fea, scores = model(tenCrop_fea)\n","        #score = (ncrop,1,nclip)\n","    scores = scores.to(\"cpu\")\n","    scores = torch.squeeze(scores)\n","    #score = (ncrop,nclip)\n","    #================= mean ======================\n","    scores = torch.mean(scores, dim=0)\n","    \n","    return scores #scores = (nclip,)\n","\n","def writeVideo(video, scores):\n","    scores = np.round(scores, 4)\n","    val, idx = torch.topk(torch.tensor(scores), 5)\n","    val = val.numpy()\n","    scores = np.repeat(np.array(scores), 16) #from clip-level score to frame-level score\n","    cap = cv.VideoCapture(video)\n","    \n","    if (cap.isOpened() == False): \n","        print(\"Unable to read camera feed\")\n","    \n","    frame_width = int(cap.get(3))\n","    frame_height = int(cap.get(4))\n","    \n","    # Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n","    writer = cv.VideoWriter('./outputVideo.mp4', cv.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\n","    currentFrame = 0\n","    while(True):\n","        ret, frame = cap.read()\n","        currentFrame = currentFrame + 1\n","        if currentFrame > len(scores):\n","            break\n","        \n","        if ret == True: \n","            #============ put background text =================\n","            text = \"Frame:\"+str(currentFrame)\n","            text_top_left = (3, 15)\n","            font = cv.FONT_HERSHEY_SIMPLEX\n","            fontScale = 0.6\n","            color = (0,0,0)\n","            thickness = 2\n","            cv.putText(frame, text, text_top_left, font, fontScale, color, thickness, cv.LINE_AA, False)\n","            #============= put Frame:1 =============\n","            text = \"Frame:\"+str(currentFrame)\n","            text_top_left = (3, 15)\n","            font = cv.FONT_HERSHEY_SIMPLEX\n","            fontScale = 0.6\n","            color = (0,255,0)\n","            thickness = 1\n","            cv.putText(frame, text, text_top_left, font, fontScale, color, thickness, cv.LINE_AA, False)\n","            #============ put background text ===========\n","            text = \"Score:\"+str(scores[int(currentFrame-1)])\n","            text_top_left = (3, 30)\n","            font = cv.FONT_HERSHEY_SIMPLEX\n","            fontScale = 0.6\n","            color = (0,0,0)\n","            thickness = 2\n","            cv.putText(frame, text, text_top_left, font, fontScale, color, thickness, cv.LINE_AA, False)\n","            #============= put Score:1 =============\n","            text = \"Score:\"+str(scores[int(currentFrame-1)])\n","            text_top_left = (3, 30)\n","            font = cv.FONT_HERSHEY_SIMPLEX\n","            fontScale = 0.6\n","            #if scores[int(currentFrame-1)] in val:\n","            if scores[int(currentFrame-1)] > 0.5:\n","                color = (0,0,255)\n","            else:\n","                color = (0,255,0)\n","            thickness = 1\n","            cv.putText(frame, text, text_top_left, font, fontScale, color, thickness, cv.LINE_AA, False)\n","\n","            #============ write frame ==============\n","            writer.write(frame)\n","        else:\n","            break \n","    \n","    # When everything done, release the video capture and video write objects\n","    cap.release()\n","    writer.release()\n","\n","def plot(pred_score):\n","    \"\"\"plot graph for a given scores, annotation\"\"\"\n","   \n","    pred_score = np.repeat(np.array(pred_score), 16) #change predicted score from clip-level to frame-level\n","    x = [(i+1) for i in range(pred_score.shape[0])]\n","    \n","    fig, ax = plt.subplots()\n","    plt.plot(x, pred_score.tolist()) #label=\"Predicted Anomaly Score\"\n","    ax.set_ylim(0, 1)\n","    plt.ylabel(\"Anomaly Score\")\n","    plt.title('Anomaly Score VS Frame Number')\n","    plt.xlabel(\"Frame Number\")\n","    #plt.legend() #show label\n","    #plt.show()\n","    plt.savefig('./my_plot.png')\n","    fig = cv.imread('./my_plot.png')\n","    return fig\n","\n","def video_identity(video):\n","    anomaly_score = predict_10crop(video)\n","    fig = plot(anomaly_score)\n","    writeVideo(video, anomaly_score)\n","    video = \"./outputVideo.mp4\"\n","    return video, fig\n"],"metadata":{"id":"8vzKTelGvzxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#gradio interface might have problem to predict anomaly score for long duration video. The solution is to run the code below.\n","#video_identity(\"./Arson016_x264.mp4\") #predict anomaly score without gradio interface"],"metadata":{"id":"sHVHPF-jWzK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo = gr.Interface(video_identity, \n","                    gr.Video(), \n","                    [\"playable_video\", \"image\"], \n","                    cache_examples=True,\n","                    allow_flagging = \"never\") \n","\n","if __name__ == \"__main__\":\n","    demo.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"id":"pTuyT1XCWxsi","executionInfo":{"status":"ok","timestamp":1683093949366,"user_tz":-480,"elapsed":2138,"user":{"displayName":"kianyooou gan","userId":"08554226229202021090"}},"outputId":"20d6a8b9-5938-42bd-8f33-f1d6e2e3f7ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7860, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"xa0AfRiMv0uk"},"execution_count":null,"outputs":[]}]}